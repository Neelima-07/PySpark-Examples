{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.master(\"local[1]\") \\\n                    .appName('SparkByExamples.com') \\\n                    .getOrCreate()\n\ndata = [(\"James\",\"Smith\",\"USA\",\"CA\"),(\"Michael\",\"Rose\",\"USA\",\"NY\"), \\\n    (\"Robert\",\"Williams\",\"USA\",\"CA\"),(\"Maria\",\"Jones\",\"USA\",\"FL\") \\\n  ]\ncolumns=[\"firstname\",\"lastname\",\"country\",\"state\"]\ndf=spark.createDataFrame(data=data,schema=columns)\ndf.show()\nprint(df.collect())\n\nstates1=df.rdd.map(lambda x: x[3]).collect()\nprint(states1)\n#['CA', 'NY', 'CA', 'FL']\nfrom collections import OrderedDict \nres = list(OrderedDict.fromkeys(states1)) \nprint(res)\n#['CA', 'NY', 'FL']\n\n\n#Example 2\nstates2=df.rdd.map(lambda x: x.state).collect()\nprint(states2)\n#['CA', 'NY', 'CA', 'FL']\n\nstates3=df.select(df.state).collect()\nprint(states3)\n#[Row(state='CA'), Row(state='NY'), Row(state='CA'), Row(state='FL')]\n\nstates4=df.select(df.state).rdd.flatMap(lambda x: x).collect()\nprint(states4)\n#['CA', 'NY', 'CA', 'FL']\n\nstates5=df.select(df.state).toPandas()['state']\nstates6=list(states5)\nprint(states6)\n#['CA', 'NY', 'CA', 'FL']\n\npandDF=df.select(df.state,df.firstname).toPandas()\nprint(list(pandDF['state']))\nprint(list(pandDF['firstname']))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"03e1b624-0f8b-4899-b2af-efc0a24d3586","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+--------+-------+-----+\n|firstname|lastname|country|state|\n+---------+--------+-------+-----+\n|    James|   Smith|    USA|   CA|\n|  Michael|    Rose|    USA|   NY|\n|   Robert|Williams|    USA|   CA|\n|    Maria|   Jones|    USA|   FL|\n+---------+--------+-------+-----+\n\n[Row(firstname='James', lastname='Smith', country='USA', state='CA'), Row(firstname='Michael', lastname='Rose', country='USA', state='NY'), Row(firstname='Robert', lastname='Williams', country='USA', state='CA'), Row(firstname='Maria', lastname='Jones', country='USA', state='FL')]\n['CA', 'NY', 'CA', 'FL']\n['CA', 'NY', 'FL']\n['CA', 'NY', 'CA', 'FL']\n[Row(state='CA'), Row(state='NY'), Row(state='CA'), Row(state='FL')]\n['CA', 'NY', 'CA', 'FL']\n['CA', 'NY', 'CA', 'FL']\n['CA', 'NY', 'CA', 'FL']\n['James', 'Michael', 'Robert', 'Maria']\n"]}],"execution_count":0},{"cell_type":"code","source":["#states1=df.rdd.map(lambda x: x[3]).collect()            #to convert into list by using RDD property\n#res = list(OrderedDict.fromkeys(states1))              #to remove duplicates and convert back to list\n#states3=df.select(df.state).collect()                      #new data frame that contains only state column\n#res = list(OrderedDict.fromkeys(states1))             #to remove duplicates and convert back to list\n#states3=df.select(df.state).collect()                      #new data frame that contains only state column\n#states5=df.select(df.state).toPandas()['state']      #to convert Spark DF to Pandas DF and state returns a pandas series\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a6fc19bd-c385-4deb-b9e7-38171ccb15d7","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#The code begins by importing the necessary libraries and creating a SparkSession with the specified configuration.\n\n#The data list contains tuples representing rows of data, and the columns list defines the column names.\n\n#The DataFrame df is created using spark.createDataFrame(data=data, schema=columns).\n\n#The show() method is called on the DataFrame to display its contents.\n\n#The collect() method is called on the DataFrame to retrieve all the rows as a list of Row objects, and it is printed.\n\n#The first approach to extract the \"state\" column is by using RDD operations. The map() transformation is applied to the RDD to extract the values of the \"state\" column, and the collect() action retrieves the values as a list. The resulting list is printed.\n\n#The second approach is similar to the first one, but instead of using the RDD operations, the DataFrame column is directly accessed using the dot notation (x.state). The resulting list is printed.\n\n#The third approach uses the select() method to select the \"state\" column from the DataFrame. The collect() action retrieves the rows as a list of Row objects, and the resulting list is printed.\n\n#The fourth approach is similar to the third one, but it flattens the list of Row objects using the flatMap() transformation. The resulting list of states is printed.\n\n#The fifth approach converts the \"state\" column of the DataFrame into a Pandas DataFrame column using toPandas(). The resulting Pandas DataFrame is accessed using dictionary-like notation (pandDF['state']), and the column values are converted to a list and printed.\n\n#The sixth approach combines the fifth approach with converting the Pandas DataFrame column to a Python list directly using list(states5).\n\n#The last part of the code demonstrates the extraction of multiple columns into a Pandas DataFrame. The selected columns are converted to a Pandas DataFrame using toPandas(), and then individual columns are accessed using dictionary-like notation (pandDF['state'], pandDF['firstname']). The values of each column are converted to lists and printed.\n\n#Overall, the code showcases various methods to extract specific columns from a PySpark DataFrame and convert them into Python lists or Pandas DataFrame columns. These approaches provide flexibility in working with specific columns or subsets of data in different formats.\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5b5fdf0e-9880-4309-b9e9-008aae679b90","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"convert-column-python-list.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
