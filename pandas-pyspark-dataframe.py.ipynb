{"cells":[{"cell_type":"code","source":["import pandas as pd    \ndata = [['Scott', 50], ['Jeff', 45], ['Thomas', 54],['Ann',34]] \n  \n# Create the pandas DataFrame \npandasDF = pd.DataFrame(data, columns = ['Name', 'Age']) \n  \n# print dataframe. \nprint(pandasDF)\n\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .master(\"local[1]\") \\\n    .appName(\"SparkByExamples.com\") \\\n    .getOrCreate()\n\nsparkDF=spark.createDataFrame(pandasDF) \nsparkDF.printSchema()\nsparkDF.show()\n\n#sparkDF=spark.createDataFrame(pandasDF.astype(str)) \nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\nmySchema = StructType([ StructField(\"First Name\", StringType(), True)\\\n                       ,StructField(\"Age\", IntegerType(), True)])\n\nsparkDF2 = spark.createDataFrame(pandasDF,schema=mySchema)\nsparkDF2.printSchema()\nsparkDF2.show()\n\n\nspark.conf.set(\"spark.sql.execution.arrow.enabled\",\"true\")\nspark.conf.set(\"spark.sql.execution.arrow.pyspark.fallback.enabled\",\"true\")\n\npandasDF2=sparkDF2.select(\"*\").toPandas\nprint(pandasDF2)\n\n\ntest=spark.conf.get(\"spark.sql.execution.arrow.enabled\")\nprint(test)\n\ntest123=spark.conf.get(\"spark.sql.execution.arrow.pyspark.fallback.enabled\")\nprint(test123)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"5147b5ef-7109-423b-9305-07e16544d554","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/plain":[],"application/vnd.databricks.v1+bamboolib_hint":"{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}"}},{"output_type":"stream","output_type":"stream","name":"stdout","text":["     Name  Age\n0   Scott   50\n1    Jeff   45\n2  Thomas   54\n3     Ann   34\nroot\n |-- Name: string (nullable = true)\n |-- Age: long (nullable = true)\n\n+------+---+\n|  Name|Age|\n+------+---+\n| Scott| 50|\n|  Jeff| 45|\n|Thomas| 54|\n|   Ann| 34|\n+------+---+\n\nroot\n |-- First Name: string (nullable = true)\n |-- Age: integer (nullable = true)\n\n+----------+---+\n|First Name|Age|\n+----------+---+\n|     Scott| 50|\n|      Jeff| 45|\n|    Thomas| 54|\n|       Ann| 34|\n+----------+---+\n\n<bound method PandasConversionMixin.toPandas of DataFrame[First Name: string, Age: int]>\ntrue\ntrue\n"]}],"execution_count":0},{"cell_type":"code","source":["#pandasDF = pd.DataFrame(data, columns = ['Name', 'Age'])          #create a pandas DF\n#sparkDF=spark.createDataFrame(pandasDF)                 #convert spark DF\n#spark.conf.set(\"spark.sql.execution.arrow.enabled\",\"true\")\n#spark.conf.set(\"spark.sql.execution.arrow.pyspark.fallback.enabled\",\"true\")          #enabling fallback mechanism in case arrow optimization fails in Pyspark\n#test=spark.conf.get(\"spark.sql.execution.arrow.enabled\")                #to find arrow optimization\n#test123=spark.conf.get(\"spark.sql.execution.arrow.pyspark.fallback.enabled\")        #to check fallback arrow optimization enabled or not\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"64e74a52-ec30-4fb9-8226-ef9e91978789","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Import the necessary libraries, including pandas and pyspark.sql.\n#Create a Pandas DataFrame pandasDF with columns 'Name' and 'Age'.\n#Print the Pandas DataFrame to display its contents.\n#Import the SparkSession class from pyspark.sql to create a Spark session.\n#Create a PySpark DataFrame sparkDF from the Pandas DataFrame using spark.createDataFrame(pandasDF).\n#Print the schema of the PySpark DataFrame using sparkDF.printSchema().\n#Show the contents of the PySpark DataFrame using sparkDF.show().\n#Define a custom schema mySchema using pyspark.sql.types.StructType with column names and types.\n#Create a new PySpark DataFrame sparkDF2 by passing the Pandas DataFrame and the custom schema to spark.createDataFrame().\n#Print the schema of the new PySpark DataFrame using sparkDF2.printSchema().\n#Show the contents of the new PySpark DataFrame using sparkDF2.show().\n#Set Spark configuration properties to enable Arrow optimization for Pandas conversion.\n#Use sparkDF2.select(\"*\").toPandas() to convert the PySpark DataFrame to a Pandas DataFrame pandasDF2.\n#Print the Pandas DataFrame pandasDF2.\n#Get the value of the Spark configuration property \"spark.sql.execution.arrow.enabled\" and print it.\n#Get the value of the Spark configuration property \"spark.sql.execution.arrow.pyspark.fallback.enabled\" and print it.\n#Please note that in your code, you missed adding parentheses () to toPandas() method. It should be toPandas(), not toPandas.\n\n#Also, make sure you have installed and set up Apache Spark correctly to run the PySpark code."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"951ab6e3-cfd7-444d-a419-2eeb902fd913","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pandas-pyspark-dataframe.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
