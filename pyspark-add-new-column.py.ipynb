{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n                    .appName('SparkByExamples.com') \\\n                    .getOrCreate()\n\ndata = [('James','Smith','M',3000),\n  ('Anna','Rose','F',4100),\n  ('Robert','Williams','M',6200), \n]\n\ncolumns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\ndf = spark.createDataFrame(data=data, schema = columns)\ndf.show()\n\n\nif 'salary1' not in df.columns:\n    print(\"aa\")\n    \n# Add new constanct column\nfrom pyspark.sql.functions import lit\ndf.withColumn(\"bonus_percent\", lit(0.3)) \\\n  .show()\n  \n#Add column from existing column\ndf.withColumn(\"bonus_amount\", df.salary*0.3) \\\n  .show()\n\n#Add column by concatinating existing columns\nfrom pyspark.sql.functions import concat_ws\ndf.withColumn(\"name\", concat_ws(\",\",\"firstname\",'lastname')) \\\n  .show()\n\n#Add current date\nfrom pyspark.sql.functions import current_date\ndf.withColumn(\"current_date\", current_date()) \\\n  .show()\n\n\nfrom pyspark.sql.functions import when\ndf.withColumn(\"grade\", \\\n   when((df.salary < 4000), lit(\"A\")) \\\n     .when((df.salary >= 4000) & (df.salary <= 5000), lit(\"B\")) \\\n     .otherwise(lit(\"C\")) \\\n  ).show()\n    \n# Add column using select\ndf.select(\"firstname\",\"salary\", lit(0.3).alias(\"bonus\")).show()\ndf.select(\"firstname\",\"salary\", lit(df.salary * 0.3).alias(\"bonus_amount\")).show()\ndf.select(\"firstname\",\"salary\", current_date().alias(\"today_date\")).show()\n\n#Add columns using SQL\ndf.createOrReplaceTempView(\"PER\")\nspark.sql(\"select firstname,salary, '0.3' as bonus from PER\").show()\nspark.sql(\"select firstname,salary, salary * 0.3 as bonus_amount from PER\").show()\nspark.sql(\"select firstname,salary, current_date() as today_date from PER\").show()\nspark.sql(\"select firstname,salary, \" +\n          \"case salary when salary < 4000 then 'A' \"+\n          \"else 'B' END as grade from PER\").show()\n\n\n\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"8d547a7d-138f-4eeb-9a7b-c682d3247662","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+--------+------+------+\n|firstname|lastname|gender|salary|\n+---------+--------+------+------+\n|    James|   Smith|     M|  3000|\n|     Anna|    Rose|     F|  4100|\n|   Robert|Williams|     M|  6200|\n+---------+--------+------+------+\n\naa\n+---------+--------+------+------+-------------+\n|firstname|lastname|gender|salary|bonus_percent|\n+---------+--------+------+------+-------------+\n|    James|   Smith|     M|  3000|          0.3|\n|     Anna|    Rose|     F|  4100|          0.3|\n|   Robert|Williams|     M|  6200|          0.3|\n+---------+--------+------+------+-------------+\n\n+---------+--------+------+------+------------+\n|firstname|lastname|gender|salary|bonus_amount|\n+---------+--------+------+------+------------+\n|    James|   Smith|     M|  3000|       900.0|\n|     Anna|    Rose|     F|  4100|      1230.0|\n|   Robert|Williams|     M|  6200|      1860.0|\n+---------+--------+------+------+------------+\n\n+---------+--------+------+------+---------------+\n|firstname|lastname|gender|salary|           name|\n+---------+--------+------+------+---------------+\n|    James|   Smith|     M|  3000|    James,Smith|\n|     Anna|    Rose|     F|  4100|      Anna,Rose|\n|   Robert|Williams|     M|  6200|Robert,Williams|\n+---------+--------+------+------+---------------+\n\n+---------+--------+------+------+------------+\n|firstname|lastname|gender|salary|current_date|\n+---------+--------+------+------+------------+\n|    James|   Smith|     M|  3000|  2023-06-19|\n|     Anna|    Rose|     F|  4100|  2023-06-19|\n|   Robert|Williams|     M|  6200|  2023-06-19|\n+---------+--------+------+------+------------+\n\n+---------+--------+------+------+-----+\n|firstname|lastname|gender|salary|grade|\n+---------+--------+------+------+-----+\n|    James|   Smith|     M|  3000|    A|\n|     Anna|    Rose|     F|  4100|    B|\n|   Robert|Williams|     M|  6200|    C|\n+---------+--------+------+------+-----+\n\n+---------+------+-----+\n|firstname|salary|bonus|\n+---------+------+-----+\n|    James|  3000|  0.3|\n|     Anna|  4100|  0.3|\n|   Robert|  6200|  0.3|\n+---------+------+-----+\n\n+---------+------+------------+\n|firstname|salary|bonus_amount|\n+---------+------+------------+\n|    James|  3000|       900.0|\n|     Anna|  4100|      1230.0|\n|   Robert|  6200|      1860.0|\n+---------+------+------------+\n\n+---------+------+----------+\n|firstname|salary|today_date|\n+---------+------+----------+\n|    James|  3000|2023-06-19|\n|     Anna|  4100|2023-06-19|\n|   Robert|  6200|2023-06-19|\n+---------+------+----------+\n\n+---------+------+-----+\n|firstname|salary|bonus|\n+---------+------+-----+\n|    James|  3000|  0.3|\n|     Anna|  4100|  0.3|\n|   Robert|  6200|  0.3|\n+---------+------+-----+\n\n+---------+------+------------+\n|firstname|salary|bonus_amount|\n+---------+------+------------+\n|    James|  3000|       900.0|\n|     Anna|  4100|      1230.0|\n|   Robert|  6200|      1860.0|\n+---------+------+------------+\n\n+---------+------+----------+\n|firstname|salary|today_date|\n+---------+------+----------+\n|    James|  3000|2023-06-19|\n|     Anna|  4100|2023-06-19|\n|   Robert|  6200|2023-06-19|\n+---------+------+----------+\n\n+---------+------+-----+\n|firstname|salary|grade|\n+---------+------+-----+\n|    James|  3000|    B|\n|     Anna|  4100|    B|\n|   Robert|  6200|    B|\n+---------+------+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#df.withColumn(\"bonus_percent\", lit(0.3)) \\                  #adds a new column and lit(0.3) provides a literal value\n\n#df.withColumn(\"name\", concat_ws(\",\",\"firstname\",'lastname')) \\                               #concate with separator\n\n#.when((df.salary >= 4000) & (df.salary <= 5000), lit(\"B\")) \\                           #create new column and check the condition\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"422ac0e1-b70e-4890-93dd-9de06f340b90","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Import the necessary libraries, including SparkSession from pyspark.sql.\n#Create a Spark session using SparkSession.builder.appName('SparkByExamples.com').getOrCreate().\n#Define a list of tuples data representing the data for the DataFrame.\n#Define the column names as a list columns.\n#Create a PySpark DataFrame df using spark.createDataFrame(data=data, schema=columns).\n#Show the contents of the DataFrame using df.show().\n#Check if a column named 'salary1' exists in the DataFrame using an if condition and df.columns.\n#Add a new constant column 'bonus_percent' with a fixed value of 0.3 to the DataFrame using lit() and df.withColumn().\n#Show the DataFrame with the newly added column using df.show().\n#Add a new column 'bonus_amount' by performing a calculation based on an existing column ('salary') using df.withColumn().\n#Show the DataFrame with the newly added column using df.show().\n#Add a new column 'name' by concatenating two existing columns ('firstname' and 'lastname') using concat_ws() and df.withColumn().\n#Show the DataFrame with the newly added column using df.show().\n#Add a new column 'current_date' with the current date using current_date() and df.withColumn().\n#Show the DataFrame with the newly added column using df.show().\n#Add a new column 'grade' based on conditions using when(), otherwise(), and df.withColumn().\n#Show the DataFrame with the newly added column using df.show().\n#Add new columns using select() with lit() and column operations.\n#Show the selected columns with the newly added columns using df.show().\n#Add new columns using SQL queries on the DataFrame by creating a temporary view using createOrReplaceTempView() and querying the view with spark.sql().\n#Show the SQL query results with the newly added columns using df.show().\n#The code demonstrates different methods to add columns to a PySpark DataFrame, including adding constant columns, performing calculations on existing columns, concatenating columns, adding the current date, and using SQL queries.\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"af31dad9-a3fd-423e-8881-1267afc0105d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-add-new-column.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
