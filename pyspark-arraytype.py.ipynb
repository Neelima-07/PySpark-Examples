{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StringType, ArrayType,StructType,StructField\nspark = SparkSession.builder \\\n                    .appName('SparkByExamples.com') \\\n                    .getOrCreate()\n\n\narrayCol = ArrayType(StringType(),False)\n\ndata = [\n (\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"],\"OH\",\"CA\"),\n (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"],\"NY\",\"NJ\"),\n (\"Robert,,Williams\",[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"],\"UT\",\"NV\")\n]\n\nschema = StructType([ \n    StructField(\"name\",StringType(),True), \n    StructField(\"languagesAtSchool\",ArrayType(StringType()),True), \n    StructField(\"languagesAtWork\",ArrayType(StringType()),True), \n    StructField(\"currentState\", StringType(), True), \n    StructField(\"previousState\", StringType(), True) \n  ])\n\ndf = spark.createDataFrame(data=data,schema=schema)\ndf.printSchema()\ndf.show()\n\nfrom pyspark.sql.functions import explode\ndf.select(df.name,explode(df.languagesAtSchool)).show()\n\nfrom pyspark.sql.functions import split\ndf.select(split(df.name,\",\").alias(\"nameAsArray\")).show()\n\nfrom pyspark.sql.functions import array\ndf.select(df.name,array(df.currentState,df.previousState).alias(\"States\")).show()\n\nfrom pyspark.sql.functions import array_contains\ndf.select(df.name,array_contains(df.languagesAtSchool,\"Java\")\n    .alias(\"array_contains\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"7d82db65-41f1-4c88-a778-8e84c99f62e0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name: string (nullable = true)\n |-- languagesAtSchool: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- languagesAtWork: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- currentState: string (nullable = true)\n |-- previousState: string (nullable = true)\n\n+----------------+------------------+---------------+------------+-------------+\n|            name| languagesAtSchool|languagesAtWork|currentState|previousState|\n+----------------+------------------+---------------+------------+-------------+\n|    James,,Smith|[Java, Scala, C++]|  [Spark, Java]|          OH|           CA|\n|   Michael,Rose,|[Spark, Java, C++]|  [Spark, Java]|          NY|           NJ|\n|Robert,,Williams|      [CSharp, VB]|[Spark, Python]|          UT|           NV|\n+----------------+------------------+---------------+------------+-------------+\n\n+----------------+------+\n|            name|   col|\n+----------------+------+\n|    James,,Smith|  Java|\n|    James,,Smith| Scala|\n|    James,,Smith|   C++|\n|   Michael,Rose,| Spark|\n|   Michael,Rose,|  Java|\n|   Michael,Rose,|   C++|\n|Robert,,Williams|CSharp|\n|Robert,,Williams|    VB|\n+----------------+------+\n\n+--------------------+\n|         nameAsArray|\n+--------------------+\n|    [James, , Smith]|\n|   [Michael, Rose, ]|\n|[Robert, , Williams]|\n+--------------------+\n\n+----------------+--------+\n|            name|  States|\n+----------------+--------+\n|    James,,Smith|[OH, CA]|\n|   Michael,Rose,|[NY, NJ]|\n|Robert,,Williams|[UT, NV]|\n+----------------+--------+\n\n+----------------+--------------+\n|            name|array_contains|\n+----------------+--------------+\n|    James,,Smith|          true|\n|   Michael,Rose,|          true|\n|Robert,,Williams|         false|\n+----------------+--------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#Import the necessary modules from the pyspark.sql package.\n#Create a SparkSession named 'SparkByExamples.com'.\n#Define the data and schema for the DataFrame. The schema defines the column names, types, and whether they can be null.\n#Create the DataFrame df using the provided data and schema.\n#Print the schema of the DataFrame using the printSchema() method.\n#Display the contents of the DataFrame using the show() method.\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2bdb83a6-6e96-421b-9f97-d47aee0dcba8","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Import the explode function from pyspark.sql.functions.\n\n#Use df.select() with explode(df.languagesAtSchool) to explode the \"languagesAtSchool\" array column into separate rows while retaining the \"name\" column. The result is displayed using show().\n\n#Import the split function from pyspark.sql.functions.\n\n#Use df.select() with split(df.name, \",\") to split the \"name\" column by commas and alias the result as \"nameAsArray\". The result is displayed using show().\n#Import the array function from pyspark.sql.functions.\n\n#Use df.select() with array(df.currentState, df.previousState).alias(\"States\") to create a new column called \"States\" that contains an array of \"currentState\" and \"previousState\" values. The \"name\" column is also retained. The result is displayed using show().\n\n#Import the array_contains function from pyspark.sql.functions.\n\n#Use df.select() with array_contains(df.languagesAtSchool, \"Java\").alias(\"array_contains\") to check if the \"languagesAtSchool\" array column contains the value \"Java\". The \"name\" column is also retained. The result is displayed using show().\n\n#These operations demonstrate how to work with arrays in a PySpark DataFrame, such as exploding arrays, splitting strings into arrays, creating new arrays, and checking array membership."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9f666c91-edc9-452d-94a7-253c08b28f67","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-arraytype.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
