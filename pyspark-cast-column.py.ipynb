{"cells":[{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\nsimpleData = [(\"James\",34,\"2006-01-01\",\"true\",\"M\",3000.60),\n    (\"Michael\",33,\"1980-01-10\",\"true\",\"F\",3300.80),\n    (\"Robert\",37,\"06-01-1992\",\"false\",\"M\",5000.50)\n  ]\n\ncolumns = [\"firstname\",\"age\",\"jobStartDate\",\"isGraduated\",\"gender\",\"salary\"]\ndf = spark.createDataFrame(data = simpleData, schema = columns)\ndf.printSchema()\ndf.show(truncate=False)\n\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.types import StringType,BooleanType,DateType\ndf2 = df.withColumn(\"age\",col(\"age\").cast(StringType())) \\\n    .withColumn(\"isGraduated\",col(\"isGraduated\").cast(BooleanType())) \\\n    .withColumn(\"jobStartDate\",col(\"jobStartDate\").cast(DateType()))\ndf2.printSchema()\n\ndf3 = df2.selectExpr(\"cast(age as int) age\",\n    \"cast(isGraduated as string) isGraduated\",\n    \"cast(jobStartDate as string) jobStartDate\")\ndf3.printSchema()\ndf3.show(truncate=False)\n\ndf3.createOrReplaceTempView(\"CastExample\")\ndf4 = spark.sql(\"SELECT STRING(age),BOOLEAN(isGraduated),DATE(jobStartDate) from CastExample\")\ndf4.printSchema()\ndf4.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"11fb420c-46f1-40ba-bf30-043702ac2a87","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- firstname: string (nullable = true)\n |-- age: long (nullable = true)\n |-- jobStartDate: string (nullable = true)\n |-- isGraduated: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: double (nullable = true)\n\n+---------+---+------------+-----------+------+------+\n|firstname|age|jobStartDate|isGraduated|gender|salary|\n+---------+---+------------+-----------+------+------+\n|James    |34 |2006-01-01  |true       |M     |3000.6|\n|Michael  |33 |1980-01-10  |true       |F     |3300.8|\n|Robert   |37 |06-01-1992  |false      |M     |5000.5|\n+---------+---+------------+-----------+------+------+\n\nroot\n |-- firstname: string (nullable = true)\n |-- age: string (nullable = true)\n |-- jobStartDate: date (nullable = true)\n |-- isGraduated: boolean (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: double (nullable = true)\n\nroot\n |-- age: integer (nullable = true)\n |-- isGraduated: string (nullable = true)\n |-- jobStartDate: string (nullable = true)\n\n+---+-----------+------------+\n|age|isGraduated|jobStartDate|\n+---+-----------+------------+\n|34 |true       |2006-01-01  |\n|33 |true       |1980-01-10  |\n|37 |false      |null        |\n+---+-----------+------------+\n\nroot\n |-- age: string (nullable = true)\n |-- isGraduated: boolean (nullable = true)\n |-- jobStartDate: date (nullable = true)\n\n+---+-----------+------------+\n|age|isGraduated|jobStartDate|\n+---+-----------+------------+\n|34 |true       |2006-01-01  |\n|33 |true       |1980-01-10  |\n|37 |false      |null        |\n+---+-----------+------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#Import the pyspark module and the SparkSession class from pyspark.sql.\n#Create a SparkSession named 'SparkByExamples.com' using SparkSession.builder.appName('SparkByExamples.com').getOrCreate().\n#Define the simple data as a list of tuples, where each tuple represents a row in the DataFrame.\n#Define the column names for the DataFrame.\n#Create the DataFrame df using the provided data and schema using spark.createDataFrame(data=simpleData, schema=columns).\n#Print the schema of the DataFrame using the printSchema() method.\n#Display the contents of the DataFrame using the show() method.\n#Import the necessary functions and data types from pyspark.sql.functions and pyspark.sql.types.\n#Use the withColumn() method to apply casting operations on the columns of the DataFrame. This converts the data types of the columns to the specified types.\n#Print the schema of the updated DataFrame df2 using the printSchema() method.\n#Use selectExpr() to cast the columns to different types using SQL-like expressions and create the DataFrame df3.\n#Print the schema of the DataFrame df3 using the printSchema() method.\n#Display the contents of the DataFrame df3 using the show() method.\n#Create a temporary view named \"CastExample\" for the DataFrame df3 using createOrReplaceTempView().\n#Use spark.sql() to run a SQL query that casts the columns to different types, and assign the result to df4.\n#Print the schema of the DataFrame df4 using the printSchema() method.\n#Display the contents of the DataFrame df4 using the show() method.\n#This code showcases how to use casting operations in PySpark to convert data types of columns in a DataFrame. It demonstrates various casting techniques such as using the withColumn() method, selectExpr(), and running SQL queries."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"80c7d13d-85f8-4533-8798-bb27503c1095","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-cast-column.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
