{"cells":[{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ndept = [(\"Finance\",10), \\\n    (\"Marketing\",20), \\\n    (\"Sales\",30), \\\n    (\"IT\",40) \\\n  ]\ndeptColumns = [\"dept_name\",\"dept_id\"]\ndeptDF = spark.createDataFrame(data=dept, schema = deptColumns)\ndeptDF.printSchema()\ndeptDF.show(truncate=False)\n\ndataCollect = deptDF.collect()\n\nprint(dataCollect)\n\ndataCollect2 = deptDF.select(\"dept_name\").collect()\nprint(dataCollect2)\n\nfor row in dataCollect:\n    print(row['dept_name'] + \",\" +str(row['dept_id']))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"b0d7152c-d1a6-4b9a-8ab0-90d20b7dc31e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|Finance  |10     |\n|Marketing|20     |\n|Sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n[Row(dept_name='Finance', dept_id=10), Row(dept_name='Marketing', dept_id=20), Row(dept_name='Sales', dept_id=30), Row(dept_name='IT', dept_id=40)]\n[Row(dept_name='Finance'), Row(dept_name='Marketing'), Row(dept_name='Sales'), Row(dept_name='IT')]\nFinance,10\nMarketing,20\nSales,30\nIT,40\n"]}],"execution_count":0},{"cell_type":"code","source":["#Import the necessary modules: pyspark and SparkSession from pyspark.sql.\n#Create a SparkSession named 'SparkByExamples.com' using SparkSession.builder.appName('SparkByExamples.com').getOrCreate().\n#Define the data for the \"dept\" DataFrame as a list of tuples, where each tuple represents a row in the DataFrame.\n#Define the column names for the \"dept\" DataFrame.\n#Create the \"deptDF\" DataFrame using the provided data and schema using spark.createDataFrame(data=dept, schema=deptColumns).\n#Print the schema of the \"deptDF\" DataFrame using the printSchema() method.\n#Display the contents of the \"deptDF\" DataFrame using the show() method.\n#Use the collect() method on the \"deptDF\" DataFrame to collect all the rows as a list of Row objects. Assign the result to the \"dataCollect\" variable.\n#Print the \"dataCollect\" variable, which contains the collected data.\n#Use the collect() method along with select() on the \"deptDF\" DataFrame to collect values from a specific column, in this case, \"dept_name\". Assign the result to the \"dataCollect2\" variable.\n#Print the \"dataCollect2\" variable, which contains the collected values from the \"dept_name\" column.\n#Iterate over the \"dataCollect\" variable, which is a list of Row objects, and access the values of the \"dept_name\" and \"dept_id\" columns using dictionary-like access (e.g., row['dept_name']). Print the values.\n#The code demonstrates two ways to collect data from a DataFrame: collecting all the rows as a list of Row objects (collect()) and collecting values from a specific column as a list (collect() with select()). The collected data can be further processed or used for analysis."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1749c427-981e-40b8-ac2d-84b0f1418657","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-collect.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
