{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ndataDictionary = [\n        ('James',{'hair':'black','eye':'brown'}),\n        ('Michael',{'hair':'brown','eye':None}),\n        ('Robert',{'hair':'red','eye':'black'}),\n        ('Washington',{'hair':'grey','eye':'grey'}),\n        ('Jefferson',{'hair':'brown','eye':''})\n        ]\n\ndf = spark.createDataFrame(data=dataDictionary, schema = ['name','properties'])\ndf.printSchema()\ndf.show(truncate=False)\n\ndf3=df.rdd.map(lambda x: \\\n    (x.name,x.properties[\"hair\"],x.properties[\"eye\"])) \\\n    .toDF([\"name\",\"hair\",\"eye\"])\ndf3.printSchema()\ndf3.show()\n\ndf.withColumn(\"hair\",df.properties.getItem(\"hair\")) \\\n  .withColumn(\"eye\",df.properties.getItem(\"eye\")) \\\n  .drop(\"properties\") \\\n  .show()\n\ndf.withColumn(\"hair\",df.properties[\"hair\"]) \\\n  .withColumn(\"eye\",df.properties[\"eye\"]) \\\n  .drop(\"properties\") \\\n  .show()\n\n# Functions\nfrom pyspark.sql.functions import explode,map_keys,col\nkeysDF = df.select(explode(map_keys(df.properties))).distinct()\nkeysList = keysDF.rdd.map(lambda x:x[0]).collect()\nkeyCols = list(map(lambda x: col(\"properties\").getItem(x).alias(str(x)), keysList))\ndf.select(df.name, *keyCols).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"5f955092-a73e-466e-8571-44742d83da2a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name: string (nullable = true)\n |-- properties: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n+----------+-----------------------------+\n|name      |properties                   |\n+----------+-----------------------------+\n|James     |{eye -> brown, hair -> black}|\n|Michael   |{eye -> null, hair -> brown} |\n|Robert    |{eye -> black, hair -> red}  |\n|Washington|{eye -> grey, hair -> grey}  |\n|Jefferson |{eye -> , hair -> brown}     |\n+----------+-----------------------------+\n\nroot\n |-- name: string (nullable = true)\n |-- hair: string (nullable = true)\n |-- eye: string (nullable = true)\n\n+----------+-----+-----+\n|      name| hair|  eye|\n+----------+-----+-----+\n|     James|black|brown|\n|   Michael|brown| null|\n|    Robert|  red|black|\n|Washington| grey| grey|\n| Jefferson|brown|     |\n+----------+-----+-----+\n\n+----------+-----+-----+\n|      name| hair|  eye|\n+----------+-----+-----+\n|     James|black|brown|\n|   Michael|brown| null|\n|    Robert|  red|black|\n|Washington| grey| grey|\n| Jefferson|brown|     |\n+----------+-----+-----+\n\n+----------+-----+-----+\n|      name| hair|  eye|\n+----------+-----+-----+\n|     James|black|brown|\n|   Michael|brown| null|\n|    Robert|  red|black|\n|Washington| grey| grey|\n| Jefferson|brown|     |\n+----------+-----+-----+\n\n+----------+-----+-----+\n|      name|  eye| hair|\n+----------+-----+-----+\n|     James|brown|black|\n|   Michael| null|brown|\n|    Robert|black|  red|\n|Washington| grey| grey|\n| Jefferson|     |brown|\n+----------+-----+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#Import the necessary module: SparkSession from pyspark.sql.\n\n#Create a SparkSession named 'SparkByExamples.com' using SparkSession.builder.appName('SparkByExamples.com').getOrCreate().\n\n#Define sample data as a list of tuples, where each tuple contains a name and a dictionary of properties.\n\n#Create a DataFrame using spark.createDataFrame(data=dataDictionary, schema=['name','properties']).\n\n#Print the schema of the DataFrame using df.printSchema().\n\n#Show the content of the DataFrame using df.show(truncate=False).\n\n#Accessing nested structure elements:\n\n#Convert the DataFrame to RDD and extract the desired nested elements using a lambda function and toDF():\n#Access nested elements using the getItem() method:\n#Access nested elements using indexing syntax:\n#Working with nested elements dynamically:\n\n#Use functions like explode, map_keys, and col from pyspark.sql.functions to extract the keys from the nested structure:\n#The code demonstrates accessing nested structure elements and dynamically working with them using PySpark DataFrame operations and functions"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a4395144-06bc-45bf-954b-8eaa4986a39c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-convert-map-to-columns.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
