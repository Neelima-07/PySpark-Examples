{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\ndata = [ (\"36636\",\"Finance\",3000,\"USA\"), \n    (\"40288\",\"Finance\",5000,\"IND\"), \n    (\"42114\",\"Sales\",3900,\"USA\"), \n    (\"39192\",\"Marketing\",2500,\"CAN\"), \n    (\"34534\",\"Sales\",6500,\"USA\") ]\nschema = StructType([\n     StructField('id', StringType(), True),\n     StructField('dept', StringType(), True),\n     StructField('salary', IntegerType(), True),\n     StructField('location', StringType(), True)\n     ])\n\ndf = spark.createDataFrame(data=data,schema=schema)\ndf.printSchema()\ndf.show(truncate=False)\n\n#Convert scolumns to Map\nfrom pyspark.sql.functions import col,lit,create_map\ndf = df.withColumn(\"propertiesMap\",create_map(\n        lit(\"salary\"),col(\"salary\"),\n        lit(\"location\"),col(\"location\")\n        )).drop(\"salary\",\"location\")\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"d30d1ed6-6f21-431c-adb7-6473b971d2d9","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- id: string (nullable = true)\n |-- dept: string (nullable = true)\n |-- salary: integer (nullable = true)\n |-- location: string (nullable = true)\n\n+-----+---------+------+--------+\n|id   |dept     |salary|location|\n+-----+---------+------+--------+\n|36636|Finance  |3000  |USA     |\n|40288|Finance  |5000  |IND     |\n|42114|Sales    |3900  |USA     |\n|39192|Marketing|2500  |CAN     |\n|34534|Sales    |6500  |USA     |\n+-----+---------+------+--------+\n\nroot\n |-- id: string (nullable = true)\n |-- dept: string (nullable = true)\n |-- propertiesMap: map (nullable = false)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n+-----+---------+---------------------------------+\n|id   |dept     |propertiesMap                    |\n+-----+---------+---------------------------------+\n|36636|Finance  |{salary -> 3000, location -> USA}|\n|40288|Finance  |{salary -> 5000, location -> IND}|\n|42114|Sales    |{salary -> 3900, location -> USA}|\n|39192|Marketing|{salary -> 2500, location -> CAN}|\n|34534|Sales    |{salary -> 6500, location -> USA}|\n+-----+---------+---------------------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#Import the necessary modules: SparkSession and StructType, StructField, StringType, IntegerType from pyspark.sql.types.\n\n#Create a SparkSession named 'SparkByExamples.com' using SparkSession.builder.appName('SparkByExamples.com').getOrCreate().\n\n#Define sample data as a list of tuples, where each tuple represents a row.\n\n#Define the schema using StructType and StructField, specifying the name and data type for each column.\n\n#Create a DataFrame using spark.createDataFrame(data=data, schema=schema).\n\n#Print the schema of the DataFrame using df.printSchema().\n\n#Show the content of the DataFrame using df.show(truncate=False).\n\n#Convert columns to a Map-type column:\n\n#Import necessary functions: col, lit, create_map from pyspark.sql.functions.\n#Use withColumn() to create a new column called \"propertiesMap\" and populate it with a map created using create_map(). Pass the column names and #column values as pairs of lit() and col() functions, respectively.\n#Drop the original \"salary\" and \"location\" columns using drop().\n#Print the updated schema using df.printSchema().\n#Show the content of the DataFrame using df.show(truncate=False).\n#The code converts the \"salary\" and \"location\" columns into a Map-type column named \"propertiesMap\" in the DataFrame. The keys of the map are fixed as \"salary\" and \"location\", while the values are taken from the corresponding columns."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b5b009e4-cd6a-4584-841c-a53304548c75","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-convert_columns-to-map.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
