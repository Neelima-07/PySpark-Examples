{"cells":[{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession, Row\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\nfrom pyspark.sql.functions import *\n\ncolumns = [\"language\",\"users_count\"]\ndata = [(\"Java\", \"20000\"), (\"Python\", \"100000\"), (\"Scala\", \"3000\")]\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\nrdd = spark.sparkContext.parallelize(data)\n\ndfFromRDD1 = rdd.toDF()\ndfFromRDD1.printSchema()\n\ndfFromRDD1 = rdd.toDF(columns)\ndfFromRDD1.printSchema()\n\ndfFromRDD2 = spark.createDataFrame(rdd).toDF(*columns)\ndfFromRDD2.printSchema()\n\ndfFromData2 = spark.createDataFrame(data).toDF(*columns)\ndfFromData2.printSchema()     \n\nrowData = map(lambda x: Row(*x), data) \ndfFromData3 = spark.createDataFrame(rowData,columns)\ndfFromData3.printSchema()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"c4f502e8-88df-4324-85e8-6ca1cd94c76d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- _1: string (nullable = true)\n |-- _2: string (nullable = true)\n\nroot\n |-- language: string (nullable = true)\n |-- users_count: string (nullable = true)\n\nroot\n |-- language: string (nullable = true)\n |-- users_count: string (nullable = true)\n\nroot\n |-- language: string (nullable = true)\n |-- users_count: string (nullable = true)\n\nroot\n |-- language: string (nullable = true)\n |-- users_count: string (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#Import the necessary modules: pyspark, SparkSession, Row, StructType, StructField, StringType, IntegerType, and functions from pyspark.sql.\n#Define the columns of the DataFrame as a list of strings: columns.\n#Define the data as a list of tuples: data.\n#Create a SparkSession named 'SparkByExamples.com' using SparkSession.builder.appName('SparkByExamples.com').getOrCreate().\n#Create an RDD from the data using spark.sparkContext.parallelize(data).\n#Convert the RDD to a DataFrame without specifying column names using rdd.toDF(). Print the schema of the resulting DataFrame using dfFromRDD1.printSchema().\n#Convert the RDD to a DataFrame by specifying column names using rdd.toDF(columns). Print the schema of the resulting DataFrame using dfFromRDD1.printSchema().\n#Create a DataFrame directly from the RDD using spark.createDataFrame(rdd).toDF(*columns). Print the schema of the resulting DataFrame using dfFromRDD2.printSchema().\n#Create a DataFrame directly from the data using spark.createDataFrame(data).toDF(*columns). Print the schema of the resulting DataFrame using dfFromData2.printSchema().\n#Create a Row object for each tuple in the data using map(lambda x: Row(*x), data).\n#Create a DataFrame from the Row objects and column names using spark.createDataFrame(rowData, columns). Print the schema of the resulting DataFrame using dfFromData3.printSchema().\n#The code demonstrates different ways to create DataFrames from RDDs and data using PySpark. Each approach offers flexibility in defining column names and data types."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a886884b-cfd7-4ce6-9d7c-7cc28cd709cf","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-create-dataframe.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
