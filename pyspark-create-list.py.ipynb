{"cells":[{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession, Row\nfrom pyspark.sql.types import StructType,StructField, StringType\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\n#Using List\ndept = [(\"Finance\",10), \n        (\"Marketing\",20), \n        (\"Sales\",30), \n        (\"IT\",40) \n      ]\n\ndeptColumns = [\"dept_name\",\"dept_id\"]\ndeptDF = spark.createDataFrame(data=dept, schema = deptColumns)\ndeptDF.printSchema()\ndeptDF.show(truncate=False)\n\ndeptSchema = StructType([       \n    StructField('firstname', StringType(), True),\n    StructField('middlename', StringType(), True),\n    StructField('lastname', StringType(), True)\n])\n\ndeptDF1 = spark.createDataFrame(data=deptcolumns, schema = deptSchema)\ndeptDF1.printSchema()\ndeptDF1.show(truncate=False)\n\n# Using list of Row type\ndept2 = [Row(\"Finance\",10), \n        Row(\"Marketing\",20), \n        Row(\"Sales\",30), \n        Row(\"IT\",40) \n      ]\n\ndeptDF2 = spark.createDataFrame(data=dept, schema = deptColumns)\ndeptDF2.printSchema()\ndeptDF2.show(truncate=False)\n\n# Convert list to RDD\nrdd = spark.sparkContext.parallelize(dept)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"dcf48d0a-8afd-4f69-8895-b36c666751ec","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|Finance  |10     |\n|Marketing|20     |\n|Sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n"]},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\nFile \u001B[0;32m<command-905482920986547>:25\u001B[0m\n\u001B[1;32m     17\u001B[0m deptDF\u001B[38;5;241m.\u001B[39mshow(truncate\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m     19\u001B[0m deptSchema \u001B[38;5;241m=\u001B[39m StructType([       \n\u001B[1;32m     20\u001B[0m     StructField(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfirstname\u001B[39m\u001B[38;5;124m'\u001B[39m, StringType(), \u001B[38;5;28;01mTrue\u001B[39;00m),\n\u001B[1;32m     21\u001B[0m     StructField(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmiddlename\u001B[39m\u001B[38;5;124m'\u001B[39m, StringType(), \u001B[38;5;28;01mTrue\u001B[39;00m),\n\u001B[1;32m     22\u001B[0m     StructField(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlastname\u001B[39m\u001B[38;5;124m'\u001B[39m, StringType(), \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     23\u001B[0m ])\n\u001B[0;32m---> 25\u001B[0m deptDF1 \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(data\u001B[38;5;241m=\u001B[39mdept, schema \u001B[38;5;241m=\u001B[39m deptSchema)\n\u001B[1;32m     26\u001B[0m deptDF1\u001B[38;5;241m.\u001B[39mprintSchema()\n\u001B[1;32m     27\u001B[0m deptDF1\u001B[38;5;241m.\u001B[39mshow(truncate\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1216\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m   1211\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_pandas \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, pd\u001B[38;5;241m.\u001B[39mDataFrame):\n\u001B[1;32m   1212\u001B[0m     \u001B[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001B[39;00m\n\u001B[1;32m   1213\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(SparkSession, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mcreateDataFrame(  \u001B[38;5;66;03m# type: ignore[call-overload]\u001B[39;00m\n\u001B[1;32m   1214\u001B[0m         data, schema, samplingRatio, verifySchema\n\u001B[1;32m   1215\u001B[0m     )\n\u001B[0;32m-> 1216\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_dataframe\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1217\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msamplingRatio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverifySchema\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[1;32m   1218\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1266\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m   1264\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromRDD(data\u001B[38;5;241m.\u001B[39mmap(prepare), schema, samplingRatio)\n\u001B[1;32m   1265\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1266\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_createFromLocal\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mmap\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprepare\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1267\u001B[0m jrdd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mSerDeUtil\u001B[38;5;241m.\u001B[39mtoJavaArray(rdd\u001B[38;5;241m.\u001B[39m_to_java_object_rdd())\n\u001B[1;32m   1268\u001B[0m jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsparkSession\u001B[38;5;241m.\u001B[39mapplySchemaToPythonRDD(jrdd\u001B[38;5;241m.\u001B[39mrdd(), struct\u001B[38;5;241m.\u001B[39mjson())\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:888\u001B[0m, in \u001B[0;36mSparkSession._createFromLocal\u001B[0;34m(self, data, schema)\u001B[0m\n\u001B[1;32m    880\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_createFromLocal\u001B[39m(\n\u001B[1;32m    881\u001B[0m     \u001B[38;5;28mself\u001B[39m, data: Iterable[Any], schema: Optional[Union[DataType, List[\u001B[38;5;28mstr\u001B[39m]]]\n\u001B[1;32m    882\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRDD[Tuple]\u001B[39m\u001B[38;5;124m\"\u001B[39m, StructType]:\n\u001B[1;32m    883\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    884\u001B[0m \u001B[38;5;124;03m    Create an RDD for DataFrame from a list or pandas.DataFrame, returns the RDD and schema.\u001B[39;00m\n\u001B[1;32m    885\u001B[0m \u001B[38;5;124;03m    This would be broken with table acl enabled as user process does not have permission to\u001B[39;00m\n\u001B[1;32m    886\u001B[0m \u001B[38;5;124;03m    write temp files.\u001B[39;00m\n\u001B[1;32m    887\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 888\u001B[0m     internal_data, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_wrap_data_schema\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    889\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39mparallelize(internal_data), struct\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:858\u001B[0m, in \u001B[0;36mSparkSession._wrap_data_schema\u001B[0;34m(self, data, schema)\u001B[0m\n\u001B[1;32m    853\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_wrap_data_schema\u001B[39m(\n\u001B[1;32m    854\u001B[0m     \u001B[38;5;28mself\u001B[39m, data: Iterable[Any], schema: Optional[Union[DataType, List[\u001B[38;5;28mstr\u001B[39m]]]\n\u001B[1;32m    855\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[Iterable[Tuple], StructType]:\n\u001B[1;32m    856\u001B[0m     \u001B[38;5;66;03m# make sure data could consumed multiple times\u001B[39;00m\n\u001B[1;32m    857\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, \u001B[38;5;28mlist\u001B[39m):\n\u001B[0;32m--> 858\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    860\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m schema \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(schema, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[1;32m    861\u001B[0m         struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inferSchemaFromList(data, names\u001B[38;5;241m=\u001B[39mschema)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1232\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe.<locals>.prepare\u001B[0;34m(obj)\u001B[0m\n\u001B[1;32m   1230\u001B[0m \u001B[38;5;129m@no_type_check\u001B[39m\n\u001B[1;32m   1231\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprepare\u001B[39m(obj):\n\u001B[0;32m-> 1232\u001B[0m     \u001B[43mverify_func\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1233\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:1867\u001B[0m, in \u001B[0;36m_make_type_verifier.<locals>.verify\u001B[0;34m(obj)\u001B[0m\n\u001B[1;32m   1865\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mverify\u001B[39m(obj: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1866\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m verify_nullability(obj):\n\u001B[0;32m-> 1867\u001B[0m         \u001B[43mverify_value\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:1838\u001B[0m, in \u001B[0;36m_make_type_verifier.<locals>.verify_struct\u001B[0;34m(obj)\u001B[0m\n\u001B[1;32m   1836\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj, (\u001B[38;5;28mtuple\u001B[39m, \u001B[38;5;28mlist\u001B[39m)):\n\u001B[1;32m   1837\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(obj) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(verifiers):\n\u001B[0;32m-> 1838\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1839\u001B[0m             new_msg(\n\u001B[1;32m   1840\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLength of object (\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m) does not match with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1841\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlength of fields (\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (\u001B[38;5;28mlen\u001B[39m(obj), \u001B[38;5;28mlen\u001B[39m(verifiers))\n\u001B[1;32m   1842\u001B[0m             )\n\u001B[1;32m   1843\u001B[0m         )\n\u001B[1;32m   1844\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m v, (_, verifier) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(obj, verifiers):\n\u001B[1;32m   1845\u001B[0m         verifier(v)\n\n\u001B[0;31mValueError\u001B[0m: Length of object (2) does not match with length of fields (3)","errorSummary":"<span class='ansi-red-fg'>ValueError</span>: Length of object (2) does not match with length of fields (3)","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n","\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n","File \u001B[0;32m<command-905482920986547>:25\u001B[0m\n","\u001B[1;32m     17\u001B[0m deptDF\u001B[38;5;241m.\u001B[39mshow(truncate\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n","\u001B[1;32m     19\u001B[0m deptSchema \u001B[38;5;241m=\u001B[39m StructType([       \n","\u001B[1;32m     20\u001B[0m     StructField(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfirstname\u001B[39m\u001B[38;5;124m'\u001B[39m, StringType(), \u001B[38;5;28;01mTrue\u001B[39;00m),\n","\u001B[1;32m     21\u001B[0m     StructField(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmiddlename\u001B[39m\u001B[38;5;124m'\u001B[39m, StringType(), \u001B[38;5;28;01mTrue\u001B[39;00m),\n","\u001B[1;32m     22\u001B[0m     StructField(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlastname\u001B[39m\u001B[38;5;124m'\u001B[39m, StringType(), \u001B[38;5;28;01mTrue\u001B[39;00m)\n","\u001B[1;32m     23\u001B[0m ])\n","\u001B[0;32m---> 25\u001B[0m deptDF1 \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(data\u001B[38;5;241m=\u001B[39mdept, schema \u001B[38;5;241m=\u001B[39m deptSchema)\n","\u001B[1;32m     26\u001B[0m deptDF1\u001B[38;5;241m.\u001B[39mprintSchema()\n","\u001B[1;32m     27\u001B[0m deptDF1\u001B[38;5;241m.\u001B[39mshow(truncate\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n","\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n","\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n","\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n","\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n","\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n","\u001B[1;32m     51\u001B[0m     )\n","\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1216\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n","\u001B[1;32m   1211\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_pandas \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, pd\u001B[38;5;241m.\u001B[39mDataFrame):\n","\u001B[1;32m   1212\u001B[0m     \u001B[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001B[39;00m\n","\u001B[1;32m   1213\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(SparkSession, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mcreateDataFrame(  \u001B[38;5;66;03m# type: ignore[call-overload]\u001B[39;00m\n","\u001B[1;32m   1214\u001B[0m         data, schema, samplingRatio, verifySchema\n","\u001B[1;32m   1215\u001B[0m     )\n","\u001B[0;32m-> 1216\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_dataframe\u001B[49m\u001B[43m(\u001B[49m\n","\u001B[1;32m   1217\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msamplingRatio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverifySchema\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n","\u001B[1;32m   1218\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1266\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n","\u001B[1;32m   1264\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromRDD(data\u001B[38;5;241m.\u001B[39mmap(prepare), schema, samplingRatio)\n","\u001B[1;32m   1265\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n","\u001B[0;32m-> 1266\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_createFromLocal\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mmap\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprepare\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m)\u001B[49m\n","\u001B[1;32m   1267\u001B[0m jrdd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mSerDeUtil\u001B[38;5;241m.\u001B[39mtoJavaArray(rdd\u001B[38;5;241m.\u001B[39m_to_java_object_rdd())\n","\u001B[1;32m   1268\u001B[0m jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsparkSession\u001B[38;5;241m.\u001B[39mapplySchemaToPythonRDD(jrdd\u001B[38;5;241m.\u001B[39mrdd(), struct\u001B[38;5;241m.\u001B[39mjson())\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:888\u001B[0m, in \u001B[0;36mSparkSession._createFromLocal\u001B[0;34m(self, data, schema)\u001B[0m\n","\u001B[1;32m    880\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_createFromLocal\u001B[39m(\n","\u001B[1;32m    881\u001B[0m     \u001B[38;5;28mself\u001B[39m, data: Iterable[Any], schema: Optional[Union[DataType, List[\u001B[38;5;28mstr\u001B[39m]]]\n","\u001B[1;32m    882\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRDD[Tuple]\u001B[39m\u001B[38;5;124m\"\u001B[39m, StructType]:\n","\u001B[1;32m    883\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n","\u001B[1;32m    884\u001B[0m \u001B[38;5;124;03m    Create an RDD for DataFrame from a list or pandas.DataFrame, returns the RDD and schema.\u001B[39;00m\n","\u001B[1;32m    885\u001B[0m \u001B[38;5;124;03m    This would be broken with table acl enabled as user process does not have permission to\u001B[39;00m\n","\u001B[1;32m    886\u001B[0m \u001B[38;5;124;03m    write temp files.\u001B[39;00m\n","\u001B[1;32m    887\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n","\u001B[0;32m--> 888\u001B[0m     internal_data, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_wrap_data_schema\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m)\u001B[49m\n","\u001B[1;32m    889\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39mparallelize(internal_data), struct\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:858\u001B[0m, in \u001B[0;36mSparkSession._wrap_data_schema\u001B[0;34m(self, data, schema)\u001B[0m\n","\u001B[1;32m    853\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_wrap_data_schema\u001B[39m(\n","\u001B[1;32m    854\u001B[0m     \u001B[38;5;28mself\u001B[39m, data: Iterable[Any], schema: Optional[Union[DataType, List[\u001B[38;5;28mstr\u001B[39m]]]\n","\u001B[1;32m    855\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[Iterable[Tuple], StructType]:\n","\u001B[1;32m    856\u001B[0m     \u001B[38;5;66;03m# make sure data could consumed multiple times\u001B[39;00m\n","\u001B[1;32m    857\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, \u001B[38;5;28mlist\u001B[39m):\n","\u001B[0;32m--> 858\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n","\u001B[1;32m    860\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m schema \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(schema, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n","\u001B[1;32m    861\u001B[0m         struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inferSchemaFromList(data, names\u001B[38;5;241m=\u001B[39mschema)\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1232\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe.<locals>.prepare\u001B[0;34m(obj)\u001B[0m\n","\u001B[1;32m   1230\u001B[0m \u001B[38;5;129m@no_type_check\u001B[39m\n","\u001B[1;32m   1231\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprepare\u001B[39m(obj):\n","\u001B[0;32m-> 1232\u001B[0m     \u001B[43mverify_func\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n","\u001B[1;32m   1233\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:1867\u001B[0m, in \u001B[0;36m_make_type_verifier.<locals>.verify\u001B[0;34m(obj)\u001B[0m\n","\u001B[1;32m   1865\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mverify\u001B[39m(obj: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n","\u001B[1;32m   1866\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m verify_nullability(obj):\n","\u001B[0;32m-> 1867\u001B[0m         \u001B[43mverify_value\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:1838\u001B[0m, in \u001B[0;36m_make_type_verifier.<locals>.verify_struct\u001B[0;34m(obj)\u001B[0m\n","\u001B[1;32m   1836\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj, (\u001B[38;5;28mtuple\u001B[39m, \u001B[38;5;28mlist\u001B[39m)):\n","\u001B[1;32m   1837\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(obj) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(verifiers):\n","\u001B[0;32m-> 1838\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n","\u001B[1;32m   1839\u001B[0m             new_msg(\n","\u001B[1;32m   1840\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLength of object (\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m) does not match with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n","\u001B[1;32m   1841\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlength of fields (\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (\u001B[38;5;28mlen\u001B[39m(obj), \u001B[38;5;28mlen\u001B[39m(verifiers))\n","\u001B[1;32m   1842\u001B[0m             )\n","\u001B[1;32m   1843\u001B[0m         )\n","\u001B[1;32m   1844\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m v, (_, verifier) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(obj, verifiers):\n","\u001B[1;32m   1845\u001B[0m         verifier(v)\n","\n","\u001B[0;31mValueError\u001B[0m: Length of object (2) does not match with length of fields (3)"]}}],"execution_count":0},{"cell_type":"code","source":["#Import the necessary modules: pyspark, SparkSession, Row, and StructType, StructField, StringType from pyspark.sql.types.\n#Create a SparkSession named 'SparkByExamples.com' using SparkSession.builder.appName('SparkByExamples.com').getOrCreate().\n#Define the department data as a list of tuples: dept.\n#Define the column names for the department DataFrame as a list of strings: deptColumns.\n#Create a DataFrame from the department data and column names using spark.createDataFrame(data=dept, schema=deptColumns). Print the schema of the resulting DataFrame using deptDF.printSchema() and display the DataFrame using deptDF.show(truncate=False).\n#Define the department schema as a StructType with three StringType fields: 'firstname', 'middlename', and 'lastname'.\n#Create a DataFrame from the department data and the custom schema using spark.createDataFrame(data=dept, schema=deptSchema). Print the schema of the resulting DataFrame using deptDF1.printSchema() and display the DataFrame using deptDF1.show(truncate=False).\n#Define the department data as a list of Row objects: dept2.\n#Create a DataFrame from the department data and column names using spark.createDataFrame(data=dept, schema=deptColumns). Print the schema of the resulting DataFrame using deptDF2.printSchema() and display the DataFrame using deptDF2.show(truncate=False).\n#Convert the department list to an RDD using spark.sparkContext.parallelize(dept).\n#The code demonstrates different ways to create DataFrames using lists and RDDs in PySpark, including defining custom schemas. Each approach provides flexibility in creating DataFrames from different types of data structures."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"28ee3a94-1cad-4108-9461-29f90897f0e5","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-create-list.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
