{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ncolumns = [\"name\",\"languagesAtSchool\",\"currentState\"]\ndata = [(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \\\n    (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"), \\\n    (\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n\ndf = spark.createDataFrame(data=data,schema=columns)\ndf.printSchema()\ndf.show(truncate=False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"5b076c9a-8b3a-422f-a4dd-0bf9c0cad95a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name: string (nullable = true)\n |-- languagesAtSchool: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- currentState: string (nullable = true)\n\n+----------------+------------------+------------+\n|name            |languagesAtSchool |currentState|\n+----------------+------------------+------------+\n|James,,Smith    |[Java, Scala, C++]|CA          |\n|Michael,Rose,   |[Spark, Java, C++]|NJ          |\n|Robert,,Williams|[CSharp, VB]      |NV          |\n+----------------+------------------+------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#from pyspark.sql import SparkSession: This line imports the SparkSession class from the pyspark.sql module. SparkSession is the entry point for interacting with Spark SQL.\n\n#spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate(): This line creates a SparkSession object named spark with the specified application name. If a SparkSession with the given name already exists, it returns that session; otherwise, it creates a new one. The SparkSession is responsible for coordinating the execution of Spark tasks.\n\n#columns = [\"name\",\"languagesAtSchool\",\"currentState\"]: This line defines a list called columns that represents the column names of the DataFrame. Each element in the list corresponds to a column in the DataFrame.\n\n#data = [(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), ...]: This line defines a list called data that contains the actual data to be loaded into the DataFrame. Each element in the list is a tuple representing a row of data. The values in the tuple correspond to the columns defined in the columns list. In this case, each row has three values: name, languagesAtSchool, and currentState.\n\n#df = spark.createDataFrame(data=data,schema=columns): This line creates a DataFrame using the createDataFrame method of the SparkSession object. It takes the data and columns variables as arguments. The schema parameter specifies the column names for the DataFrame. The createDataFrame method infers the data types of the columns based on the provided data.\n\n#df.printSchema(): This line prints the schema of the DataFrame. The schema displays the column names and their corresponding data types. It provides a structural overview of the DataFrame.\n\n#df.show(truncate=False): This line displays the contents of the DataFrame. The show method is used to print a certain number of rows from the DataFrame. By default, it shows the first 20 rows. Setting truncate=False ensures that the complete values of the columns are displayed without truncation.\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f53bba08-9310-420f-8456-295ffd03ed9f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-dataframe-flatMap.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
