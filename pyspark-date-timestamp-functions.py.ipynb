{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\n# Create SparkSession\nspark = SparkSession.builder \\\n               .appName('SparkByExamples.com') \\\n               .getOrCreate()\ndata=[[\"1\",\"2020-02-01\"],[\"2\",\"2019-03-01\"],[\"3\",\"2021-03-01\"]]\ndf=spark.createDataFrame(data,[\"id\",\"input\"])\ndf.show()\n\nfrom pyspark.sql.functions import *\n\n#current_date()\ndf.select(current_date().alias(\"current_date\")\n  ).show(1)\n\n#date_format()\ndf.select(col(\"input\"), \n    date_format(col(\"input\"), \"MM-dd-yyyy\").alias(\"date_format\") \n  ).show()\n\n#to_date()\ndf.select(col(\"input\"), \n    to_date(col(\"input\"), \"yyy-MM-dd\").alias(\"to_date\") \n  ).show()\n\n#datediff()\ndf.select(col(\"input\"), \n    datediff(current_date(),col(\"input\")).alias(\"datediff\")  \n  ).show()\n\n#months_between()\ndf.select(col(\"input\"), \n    months_between(current_date(),col(\"input\")).alias(\"months_between\")  \n  ).show()\n\n#trunc()\ndf.select(col(\"input\"), \n    trunc(col(\"input\"),\"Month\").alias(\"Month_Trunc\"), \n    trunc(col(\"input\"),\"Year\").alias(\"Month_Year\"), \n    trunc(col(\"input\"),\"Month\").alias(\"Month_Trunc\")\n   ).show()\n\n#add_months() , date_add(), date_sub()\n\ndf.select(col(\"input\"), \n    add_months(col(\"input\"),3).alias(\"add_months\"), \n    add_months(col(\"input\"),-3).alias(\"sub_months\"), \n    date_add(col(\"input\"),4).alias(\"date_add\"), \n    date_sub(col(\"input\"),4).alias(\"date_sub\") \n  ).show()\n\n#\n\ndf.select(col(\"input\"), \n     year(col(\"input\")).alias(\"year\"), \n     month(col(\"input\")).alias(\"month\"), \n     next_day(col(\"input\"),\"Sunday\").alias(\"next_day\"), \n     weekofyear(col(\"input\")).alias(\"weekofyear\") \n  ).show()\n\ndf.select(col(\"input\"),  \n     dayofweek(col(\"input\")).alias(\"dayofweek\"), \n     dayofmonth(col(\"input\")).alias(\"dayofmonth\"), \n     dayofyear(col(\"input\")).alias(\"dayofyear\"), \n  ).show()\n\ndata=[[\"1\",\"02-01-2020 11 01 19 06\"],[\"2\",\"03-01-2019 12 01 19 406\"],[\"3\",\"03-01-2021 12 01 19 406\"]]\ndf2=spark.createDataFrame(data,[\"id\",\"input\"])\ndf2.show(truncate=False)\n\n#current_timestamp()\ndf2.select(current_timestamp().alias(\"current_timestamp\")\n  ).show(1,truncate=False)\n\n#to_timestamp()\ndf2.select(col(\"input\"), \n    to_timestamp(col(\"input\"), \"MM-dd-yyyy HH mm ss SSS\").alias(\"to_timestamp\") \n  ).show(truncate=False)\n\n\n#hour, minute,second\ndata=[[\"1\",\"2020-02-01 11:01:19.06\"],[\"2\",\"2019-03-01 12:01:19.406\"],[\"3\",\"2021-03-01 12:01:19.406\"]]\ndf3=spark.createDataFrame(data,[\"id\",\"input\"])\n\ndf3.select(col(\"input\"), \n    hour(col(\"input\")).alias(\"hour\"), \n    minute(col(\"input\")).alias(\"minute\"),\n    second(col(\"input\")).alias(\"second\") \n  ).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"546cfb91-c626-44bb-982a-81c51a192072","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+----------+\n| id|     input|\n+---+----------+\n|  1|2020-02-01|\n|  2|2019-03-01|\n|  3|2021-03-01|\n+---+----------+\n\n+------------+\n|current_date|\n+------------+\n|  2023-06-19|\n+------------+\nonly showing top 1 row\n\n+----------+-----------+\n|     input|date_format|\n+----------+-----------+\n|2020-02-01| 02-01-2020|\n|2019-03-01| 03-01-2019|\n|2021-03-01| 03-01-2021|\n+----------+-----------+\n\n+----------+----------+\n|     input|   to_date|\n+----------+----------+\n|2020-02-01|2020-02-01|\n|2019-03-01|2019-03-01|\n|2021-03-01|2021-03-01|\n+----------+----------+\n\n+----------+--------+\n|     input|datediff|\n+----------+--------+\n|2020-02-01|    1234|\n|2019-03-01|    1571|\n|2021-03-01|     840|\n+----------+--------+\n\n+----------+--------------+\n|     input|months_between|\n+----------+--------------+\n|2020-02-01|   40.58064516|\n|2019-03-01|   51.58064516|\n|2021-03-01|   27.58064516|\n+----------+--------------+\n\n+----------+-----------+----------+-----------+\n|     input|Month_Trunc|Month_Year|Month_Trunc|\n+----------+-----------+----------+-----------+\n|2020-02-01| 2020-02-01|2020-01-01| 2020-02-01|\n|2019-03-01| 2019-03-01|2019-01-01| 2019-03-01|\n|2021-03-01| 2021-03-01|2021-01-01| 2021-03-01|\n+----------+-----------+----------+-----------+\n\n+----------+----------+----------+----------+----------+\n|     input|add_months|sub_months|  date_add|  date_sub|\n+----------+----------+----------+----------+----------+\n|2020-02-01|2020-05-01|2019-11-01|2020-02-05|2020-01-28|\n|2019-03-01|2019-06-01|2018-12-01|2019-03-05|2019-02-25|\n|2021-03-01|2021-06-01|2020-12-01|2021-03-05|2021-02-25|\n+----------+----------+----------+----------+----------+\n\n+----------+----+-----+----------+----------+\n|     input|year|month|  next_day|weekofyear|\n+----------+----+-----+----------+----------+\n|2020-02-01|2020|    2|2020-02-02|         5|\n|2019-03-01|2019|    3|2019-03-03|         9|\n|2021-03-01|2021|    3|2021-03-07|         9|\n+----------+----+-----+----------+----------+\n\n+----------+---------+----------+---------+\n|     input|dayofweek|dayofmonth|dayofyear|\n+----------+---------+----------+---------+\n|2020-02-01|        7|         1|       32|\n|2019-03-01|        6|         1|       60|\n|2021-03-01|        2|         1|       60|\n+----------+---------+----------+---------+\n\n+---+-----------------------+\n|id |input                  |\n+---+-----------------------+\n|1  |02-01-2020 11 01 19 06 |\n|2  |03-01-2019 12 01 19 406|\n|3  |03-01-2021 12 01 19 406|\n+---+-----------------------+\n\n+-----------------------+\n|current_timestamp      |\n+-----------------------+\n|2023-06-19 20:27:26.271|\n+-----------------------+\nonly showing top 1 row\n\n+-----------------------+-----------------------+\n|input                  |to_timestamp           |\n+-----------------------+-----------------------+\n|02-01-2020 11 01 19 06 |2020-02-01 11:01:19.06 |\n|03-01-2019 12 01 19 406|2019-03-01 12:01:19.406|\n|03-01-2021 12 01 19 406|2021-03-01 12:01:19.406|\n+-----------------------+-----------------------+\n\n+-----------------------+----+------+------+\n|input                  |hour|minute|second|\n+-----------------------+----+------+------+\n|2020-02-01 11:01:19.06 |11  |1     |19    |\n|2019-03-01 12:01:19.406|12  |1     |19    |\n|2021-03-01 12:01:19.406|12  |1     |19    |\n+-----------------------+----+------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#It imports the necessary modules: SparkSession from pyspark.sql and functions (current_date, date_format, to_date, datediff, months_between, trunc, add_months, date_add, date_sub, year, month, next_day, weekofyear, dayofweek, dayofmonth, dayofyear, current_timestamp, to_timestamp, hour, minute, second) from pyspark.sql.functions.\n#It creates a SparkSession object named spark using SparkSession.builder.appName('SparkByExamples.com').getOrCreate().\n#It creates a DataFrame df with two columns: \"id\" and \"input\", and three rows of data.\n#It uses various date and timestamp functions from pyspark.sql.functions to perform operations on the \"input\" column of the DataFrame. The functions used include current_date, date_format, to_date, datediff, months_between, trunc, add_months, date_add, date_sub, year, month, next_day, weekofyear, dayofweek, dayofmonth, dayofyear, current_timestamp, to_timestamp, hour, minute, second.\n#It shows the result of each select operation, displaying the computed values.\n#The code demonstrates how to manipulate and extract information from date and timestamp data using PySpark's built-in functions. It covers various operations such as date formatting, converting between different date and timestamp formats, calculating differences between dates, extracting specific components (year, month, day, hour, minute, second), and more."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4bf1f0a3-10ca-45cd-8ce4-2b5c0b5eddac","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-date-timestamp-functions.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
