{"cells":[{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import expr\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ndata = [(\"James\", \"Sales\", 3000), \\\n    (\"Michael\", \"Sales\", 4600), \\\n    (\"Robert\", \"Sales\", 4100), \\\n    (\"Maria\", \"Finance\", 3000), \\\n    (\"James\", \"Sales\", 3000), \\\n    (\"Scott\", \"Finance\", 3300), \\\n    (\"Jen\", \"Finance\", 3900), \\\n    (\"Jeff\", \"Marketing\", 3000), \\\n    (\"Kumar\", \"Marketing\", 2000), \\\n    (\"Saif\", \"Sales\", 4100) \\\n  ]\ncolumns= [\"employee_name\", \"department\", \"salary\"]\ndf = spark.createDataFrame(data = data, schema = columns)\ndf.printSchema()\ndf.show(truncate=False)\n\ndistinctDF = df.distinct()\nprint(\"Distinct count: \"+str(distinctDF.count()))\ndistinctDF.show(truncate=False)\n\ndf2 = df.dropDuplicates()\nprint(\"Distinct count: \"+str(df2.count()))\ndf2.show(truncate=False)\n\ndropDisDF = df.dropDuplicates([\"department\",\"salary\"])\nprint(\"Distinct count of department salary : \"+str(dropDisDF.count()))\ndropDisDF.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"a5349316-ab19-4245-bd1e-228af508116c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|James        |Sales     |3000  |\n|Michael      |Sales     |4600  |\n|Robert       |Sales     |4100  |\n|Maria        |Finance   |3000  |\n|James        |Sales     |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Jeff         |Marketing |3000  |\n|Kumar        |Marketing |2000  |\n|Saif         |Sales     |4100  |\n+-------------+----------+------+\n\nDistinct count: 9\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|James        |Sales     |3000  |\n|Michael      |Sales     |4600  |\n|Robert       |Sales     |4100  |\n|Maria        |Finance   |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Jeff         |Marketing |3000  |\n|Kumar        |Marketing |2000  |\n|Saif         |Sales     |4100  |\n+-------------+----------+------+\n\nDistinct count: 9\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|James        |Sales     |3000  |\n|Michael      |Sales     |4600  |\n|Robert       |Sales     |4100  |\n|Maria        |Finance   |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Jeff         |Marketing |3000  |\n|Kumar        |Marketing |2000  |\n|Saif         |Sales     |4100  |\n+-------------+----------+------+\n\nDistinct count of department salary : 8\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|Maria        |Finance   |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Kumar        |Marketing |2000  |\n|Jeff         |Marketing |3000  |\n|James        |Sales     |3000  |\n|Robert       |Sales     |4100  |\n|Michael      |Sales     |4600  |\n+-------------+----------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#It imports the necessary modules: SparkSession from pyspark.sql and expr from pyspark.sql.functions.\n#It creates a SparkSession object named spark using SparkSession.builder.appName('SparkByExamples.com').getOrCreate().\n#It defines a list of data tuples and a list of column names to create a DataFrame df using spark.createDataFrame(data=data, schema=columns).\n#It prints the schema and shows the content of the DataFrame using df.printSchema() and df.show(truncate=False).\n#It applies the distinct function to the DataFrame df and assigns the result to distinctDF. It prints the count of distinct rows using distinctDF.count() and shows the distinct rows using distinctDF.show(truncate=False).\n#It applies the dropDuplicates function to the DataFrame df and assigns the result to df2. It prints the count of distinct rows using df2.count() and shows the distinct rows using df2.show(truncate=False).\n#It applies the dropDuplicates function to the DataFrame df with specific columns (\"department\" and \"salary\") and assigns the result to dropDisDF. It prints the count of distinct rows using dropDisDF.count() and shows the distinct rows using dropDisDF.show(truncate=False).\n#The code demonstrates how to find distinct rows in a DataFrame using the distinct function, as well as how to remove duplicate rows using the dropDuplicates function. The dropDuplicates function can be applied to the entire DataFrame or specific columns."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"58b32333-7384-4557-9a6e-b3907c2de4ff","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-distinct.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
