{"cells":[{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\nsimpleData = ((\"James\",\"\",\"Smith\",\"36636\",\"NewYork\",3100), \\\n    (\"Michael\",\"Rose\",\"\",\"40288\",\"California\",4300), \\\n    (\"Robert\",\"\",\"Williams\",\"42114\",\"Florida\",1400), \\\n    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"Florida\",5500), \\\n    (\"Jen\",\"Mary\",\"Brown\",\"34561\",\"NewYork\",3000) \\\n  )\ncolumns= [\"firstname\",\"middlename\",\"lastname\",\"id\",\"location\",\"salary\"]\n\ndf = spark.createDataFrame(data = simpleData, schema = columns)\n\ndf.printSchema()\ndf.show(truncate=False)\n\ndf.drop(\"firstname\") \\\n  .printSchema()\n  \ndf.drop(col(\"firstname\")) \\\n  .printSchema()  \n  \ndf.drop(df.firstname) \\\n  .printSchema()\n\ndf.drop(\"firstname\",\"middlename\",\"lastname\") \\\n    .printSchema()\n\ncols = (\"firstname\",\"middlename\",\"lastname\")\n\ndf.drop(*cols) \\\n   .printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"52f12bec-e10b-4587-8077-542a004aefe9","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- id: string (nullable = true)\n |-- location: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+---------+----------+--------+-----+----------+------+\n|firstname|middlename|lastname|id   |location  |salary|\n+---------+----------+--------+-----+----------+------+\n|James    |          |Smith   |36636|NewYork   |3100  |\n|Michael  |Rose      |        |40288|California|4300  |\n|Robert   |          |Williams|42114|Florida   |1400  |\n|Maria    |Anne      |Jones   |39192|Florida   |5500  |\n|Jen      |Mary      |Brown   |34561|NewYork   |3000  |\n+---------+----------+--------+-----+----------+------+\n\nroot\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- id: string (nullable = true)\n |-- location: string (nullable = true)\n |-- salary: long (nullable = true)\n\nroot\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- id: string (nullable = true)\n |-- location: string (nullable = true)\n |-- salary: long (nullable = true)\n\nroot\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- id: string (nullable = true)\n |-- location: string (nullable = true)\n |-- salary: long (nullable = true)\n\nroot\n |-- id: string (nullable = true)\n |-- location: string (nullable = true)\n |-- salary: long (nullable = true)\n\nroot\n |-- id: string (nullable = true)\n |-- location: string (nullable = true)\n |-- salary: long (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#It imports the necessary modules: SparkSession from pyspark.sql and col from pyspark.sql.functions.\n#It creates a SparkSession object named spark using SparkSession.builder.appName('SparkByExamples.com').getOrCreate().\n#It defines a list of tuples containing sample data and a list of column names to create a DataFrame df using spark.createDataFrame(data=simpleData, schema=columns).\n#It prints the schema and shows the content of the DataFrame using df.printSchema() and df.show(truncate=False).\n#It demonstrates different ways to drop columns from the DataFrame:\n#df.drop(\"firstname\") drops the \"firstname\" column and returns a new DataFrame.\n#df.drop(col(\"firstname\")) drops the column specified by the col(\"firstname\") expression and returns a new DataFrame.\n#df.drop(df.firstname) drops the column specified by the df.firstname reference and returns a new DataFrame.\n#df.drop(\"firstname\", \"middlename\", \"lastname\") drops multiple columns specified by their names and returns a new DataFrame.\n#df.drop(*cols) drops multiple columns specified as a tuple and returns a new DataFrame.\n#For each drop operation, it prints the schema of the resulting DataFrame using .printSchema().\n#The code demonstrates various ways to drop columns from a DataFrame using the drop function. You can specify column names as strings, use col() function to reference columns, or pass multiple column names as arguments or in a tuple."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"840ffece-65ea-470a-a943-41574034a443","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-drop-column.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
