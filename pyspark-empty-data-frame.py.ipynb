{"cells":[{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType,StructField, StringType\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\nschema = StructType([\n  StructField('firstname', StringType(), True),\n  StructField('middlename', StringType(), True),\n  StructField('lastname', StringType(), True)\n  ])\ndf = spark.createDataFrame(spark.sparkContext.emptyRDD(),schema)\ndf.printSchema()\n\ndf1 = spark.sparkContext.parallelize([]).toDF(schema)\ndf1.printSchema()\n\ndf2 = spark.createDataFrame([], schema)\ndf2.printSchema()\n\ndf3 = spark.createDataFrame([], schema)\ndf3.printSchema()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"dae435e4-baef-4a51-8d2c-90f080d8bc15","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n\nroot\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n\nroot\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n\nroot\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#It imports the necessary modules: pyspark, SparkSession, and StructType, StructField from pyspark.sql.types.\n#It creates a SparkSession object named spark with the configuration appName('SparkByExamples.com').\n#It defines a schema using StructType and StructField. In this case, the schema consists of three fields: 'firstname', 'middlename', and 'lastname', all of type StringType.\n#It creates an empty DataFrame df by passing an empty RDD and the schema to spark.createDataFrame(). The emptyRDD() method is called on spark.sparkContext to create an empty RDD. The schema is provided as the second argument. The resulting DataFrame is displayed using df.printSchema().\n#It creates an empty DataFrame df1 by parallelizing an empty list and converting it to a DataFrame using .toDF(schema). The empty list is created using spark.sparkContext.parallelize([]), and the schema is provided as an argument to toDF(). The resulting DataFrame is displayed using df1.printSchema().\n#It creates an empty DataFrame df2 by passing an empty list and the schema to spark.createDataFrame(). The empty list is provided as the first argument, and the schema is provided as the second argument. The resulting DataFrame is displayed using df2.printSchema().\n#It creates an empty DataFrame df3 using spark.emptyDataFrame(). This method returns an empty DataFrame with no columns and is equivalent to using spark.createDataFrame([], schema). The resulting DataFrame is displayed using df3.printSchema().\n#The code demonstrates different ways to create an empty DataFrame with a specified schema. You can create an empty DataFrame by providing an empty RDD, parallelizing an empty list, or passing an empty list directly to spark.createDataFrame(). The resulting DataFrames will have the specified schema."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cc5d7c2a-23a8-43f4-a620-4f84158c22cc","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-empty-data-frame.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
