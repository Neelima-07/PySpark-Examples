{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\nfrom pyspark.sql.functions import expr\n#Concatenate columns\ndata=[(\"James\",\"Bond\"),(\"Scott\",\"Varsa\")] \ndf=spark.createDataFrame(data).toDF(\"col1\",\"col2\") \ndf.withColumn(\"Name\",expr(\" col1 ||','|| col2\")).show()\n\n#Using CASE WHEN sql expression\ndata = [(\"James\",\"M\"),(\"Michael\",\"F\"),(\"Jen\",\"\")]\ncolumns = [\"name\",\"gender\"]\ndf = spark.createDataFrame(data = data, schema = columns)\ndf2 = df.withColumn(\"gender\", expr(\"CASE WHEN gender = 'M' THEN 'Male' \" +\n           \"WHEN gender = 'F' THEN 'Female' ELSE 'unknown' END\"))\ndf2.show()\n\n#Add months from a value of another column\ndata=[(\"2019-01-23\",1),(\"2019-06-24\",2),(\"2019-09-20\",3)] \ndf=spark.createDataFrame(data).toDF(\"date\",\"increment\") \ndf.select(df.date,df.increment,\n     expr(\"add_months(date,increment)\")\n  .alias(\"inc_date\")).show()\n\n# Providing alias using 'as'\ndf.select(df.date,df.increment,\n     expr(\"\"\"add_months(date,increment) as inc_date\"\"\")\n  ).show()\n\n# Add\ndf.select(df.date,df.increment,\n     expr(\"increment + 5 as new_increment\")\n  ).show()\n\ndf.select(\"increment\",expr(\"cast(increment as string) as str_increment\")) \\\n  .printSchema()\n#Use expr()  to filter the rows\ndata=[(100,2),(200,3000),(500,500)] \ndf=spark.createDataFrame(data).toDF(\"col1\",\"col2\") \ndf.filter(expr(\"col1 == col2\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"22a423f8-fa45-46c9-ad75-c464684b7b63","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+-----+-----------+\n| col1| col2|       Name|\n+-----+-----+-----------+\n|James| Bond| James,Bond|\n|Scott|Varsa|Scott,Varsa|\n+-----+-----+-----------+\n\n+-------+-------+\n|   name| gender|\n+-------+-------+\n|  James|   Male|\n|Michael| Female|\n|    Jen|unknown|\n+-------+-------+\n\n+----------+---------+----------+\n|      date|increment|  inc_date|\n+----------+---------+----------+\n|2019-01-23|        1|2019-02-23|\n|2019-06-24|        2|2019-08-24|\n|2019-09-20|        3|2019-12-20|\n+----------+---------+----------+\n\n+----------+---------+----------+\n|      date|increment|  inc_date|\n+----------+---------+----------+\n|2019-01-23|        1|2019-02-23|\n|2019-06-24|        2|2019-08-24|\n|2019-09-20|        3|2019-12-20|\n+----------+---------+----------+\n\n+----------+---------+-------------+\n|      date|increment|new_increment|\n+----------+---------+-------------+\n|2019-01-23|        1|            6|\n|2019-06-24|        2|            7|\n|2019-09-20|        3|            8|\n+----------+---------+-------------+\n\nroot\n |-- increment: long (nullable = true)\n |-- str_increment: string (nullable = true)\n\n+----+----+\n|col1|col2|\n+----+----+\n| 500| 500|\n+----+----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#Concatenate columns: This section creates a DataFrame df with two columns 'col1' and 'col2', and adds a new column 'Name' by concatenating 'col1' and 'col2' using the expr function. The resulting DataFrame is displayed using df.withColumn(\"Name\",expr(\" col1 ||','|| col2\")).show().\n\n#Using CASE WHEN SQL expression: This section creates a DataFrame df with columns 'name' and 'gender'. It uses the expr function and SQL CASE WHEN expression to transform the 'gender' column values. The resulting DataFrame is displayed using df2.show().\n\n#Add months from a value of another column: This section creates a DataFrame df with columns 'date' and 'increment'. It adds a new column 'inc_date' by using the expr function and applying the add_months function to the 'date' column with the value from the 'increment' column. The resulting DataFrame is displayed using df.select().\n\n#Providing alias using 'as': This section demonstrates the usage of 'as' to provide an alias for the newly added column. It shows an alternative way to write the previous expression with the alias using expr function and select.\n\n#Add: This section adds a new column 'new_increment' by adding a constant value (5) to the 'increment' column using the expr function. The resulting DataFrame is displayed using df.select().\n\n#Using expr() to filter rows: This section filters the rows in the DataFrame based on the condition specified in expr. It selects the rows where 'col1' is equal to 'col2' using df.filter(expr(\"col1 == col2\")). The resulting DataFrame is displayed using show().\n\n#These examples showcase the flexibility of PySpark's expr function for performing column-based computations, transformations, and filtering operations within DataFrames."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"83876253-9bb6-4812-b0d5-20e50e05fbf0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-expr.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
