{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\nspark: SparkSession = SparkSession.builder \\\n    .master(\"local[1]\") \\\n    .appName(\"SparkByExamples.com\") \\\n    .getOrCreate()\n\ndata = [\n    (\"James\",None,\"M\"),\n    (\"Anna\",\"NY\",\"F\"),\n    (\"Julia\",None,None)\n]\n\ncolumns = [\"name\",\"state\",\"gender\"]\ndf =spark.createDataFrame(data,columns)\n\ndf.printSchema()\ndf.show()\n\ndf.filter(\"state is NULL\").show()\ndf.filter(df.state.isNull()).show()\ndf.filter(col(\"state\").isNull()).show()\n\ndf.filter(\"state IS NULL AND gender IS NULL\").show()\ndf.filter(df.state.isNull() & df.gender.isNull()).show()\n\ndf.filter(\"state is not NULL\").show()\ndf.filter(\"NOT state is NULL\").show()\ndf.filter(df.state.isNotNull()).show()\ndf.filter(col(\"state\").isNotNull()).show()\ndf.na.drop(subset=[\"state\"]).show()\n\ndf.createOrReplaceTempView(\"DATA\")\nspark.sql(\"SELECT * FROM DATA where STATE IS NULL\").show()\nspark.sql(\"SELECT * FROM DATA where STATE IS NULL AND GENDER IS NULL\").show()\nspark.sql(\"SELECT * FROM DATA where STATE IS NOT NULL\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"dd25a452-1fa2-4ab7-b3cf-921c31ae03ce","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name: string (nullable = true)\n |-- state: string (nullable = true)\n |-- gender: string (nullable = true)\n\n+-----+-----+------+\n| name|state|gender|\n+-----+-----+------+\n|James| null|     M|\n| Anna|   NY|     F|\n|Julia| null|  null|\n+-----+-----+------+\n\n+-----+-----+------+\n| name|state|gender|\n+-----+-----+------+\n|James| null|     M|\n|Julia| null|  null|\n+-----+-----+------+\n\n+-----+-----+------+\n| name|state|gender|\n+-----+-----+------+\n|James| null|     M|\n|Julia| null|  null|\n+-----+-----+------+\n\n+-----+-----+------+\n| name|state|gender|\n+-----+-----+------+\n|James| null|     M|\n|Julia| null|  null|\n+-----+-----+------+\n\n+-----+-----+------+\n| name|state|gender|\n+-----+-----+------+\n|Julia| null|  null|\n+-----+-----+------+\n\n+-----+-----+------+\n| name|state|gender|\n+-----+-----+------+\n|Julia| null|  null|\n+-----+-----+------+\n\n+----+-----+------+\n|name|state|gender|\n+----+-----+------+\n|Anna|   NY|     F|\n+----+-----+------+\n\n+----+-----+------+\n|name|state|gender|\n+----+-----+------+\n|Anna|   NY|     F|\n+----+-----+------+\n\n+----+-----+------+\n|name|state|gender|\n+----+-----+------+\n|Anna|   NY|     F|\n+----+-----+------+\n\n+----+-----+------+\n|name|state|gender|\n+----+-----+------+\n|Anna|   NY|     F|\n+----+-----+------+\n\n+----+-----+------+\n|name|state|gender|\n+----+-----+------+\n|Anna|   NY|     F|\n+----+-----+------+\n\n+-----+-----+------+\n| name|state|gender|\n+-----+-----+------+\n|James| null|     M|\n|Julia| null|  null|\n+-----+-----+------+\n\n+-----+-----+------+\n| name|state|gender|\n+-----+-----+------+\n|Julia| null|  null|\n+-----+-----+------+\n\n+----+-----+------+\n|name|state|gender|\n+----+-----+------+\n|Anna|   NY|     F|\n+----+-----+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#First, the code imports the necessary modules: SparkSession from pyspark.sql and the col function from pyspark.sql.functions.\n\n#The code creates a SparkSession named spark with a specified configuration. This session will be used to interact with Spark and perform various operations on the data.\n\n#The code defines the data as a list of tuples, where each tuple represents a row in the DataFrame. The columns list contains the names of the columns in the DataFrame.\n\n#Using the spark.createDataFrame() method, the code creates a DataFrame named df from the data and columns.\n\n#The df.printSchema() method is called to print the schema of the DataFrame, which shows the column names and their data types.\n\n#The df.show() method is called to display the content of the DataFrame. This will print the rows of the DataFrame in a tabular format.\n\n#The code applies filters to the DataFrame using different approaches:\n\n#df.filter(\"state is NULL\"): This filter selects rows where the \"state\" column is null. The filter condition is specified as a string using SQL syntax.\n\n#df.filter(df.state.isNull()): This filter selects rows where the \"state\" column is null. It uses the isNull() method on the column object df.state.\n\n#df.filter(col(\"state\").isNull()): This filter selects rows where the \"state\" column is null. It uses the isNull() function from the col module to create a column object.\n\n#Similar to step 7, the code applies filters to the DataFrame for specific conditions:\n\n#df.filter(\"state IS NULL AND gender IS NULL\"): This filter selects rows where both the \"state\" and \"gender\" columns are null.\n\n#df.filter(df.state.isNull() & df.gender.isNull()): This filter selects rows where both the \"state\" and \"gender\" columns are null. It uses the & operator to combine the filter conditions.\n\n#The code applies filters to select rows where the \"state\" column is not null:\n\n#df.filter(\"state is not NULL\"): This filter selects rows where the \"state\" column is not null.\n\n#df.filter(\"NOT state is NULL\"): This filter selects rows where the \"state\" column is not null. It uses the NOT operator to negate the filter condition.\n\n#df.filter(df.state.isNotNull()): This filter selects rows where the \"state\" column is not null. It uses the isNotNull() method on the column object df.state.\n\n#df.filter(col(\"state\").isNotNull()): This filter selects rows where the \"state\" column is not null. It uses the isNotNull() function from the col module to create a column object.\n\n#The df.na.drop(subset=[\"state\"]) method is called to drop rows that have null values in the \"state\" column.\n\n#The code creates a temporary view named \"DATA\" for the DataFrame using df.createOrReplaceTempView(\"DATA\"). This allows executing SQL queries on the DataFrame.\n\n#Spark SQL queries are executed using spark.sql():\n\n#spark.sql(\"SELECT * FROM DATA where STATE IS NULL\"): This query selects rows from the \"DATA\" view where the \"STATE\" column is null.\n\n#spark.sql(\"SELECT * FROM DATA where STATE IS NULL AND GENDER IS NULL\"): This query selects rows from the \"DATA\" view where both the \"STATE\""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7deb0949-4be6-45bf-9043-177328f3ce77","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-filter-null.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
