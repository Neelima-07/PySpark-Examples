{"cells":[{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ndata2 = [(1,\"James Smith\"), (2,\"Michael Rose\"),\n    (3,\"Robert Williams\"), (4,\"Rames Rose\"),(5,\"Rames rose\")\n  ]\ndf2 = spark.createDataFrame(data = data2, schema = [\"id\",\"name\"])\n\ndf2.filter(df2.name.like(\"%rose%\")).show()\ndf2.filter(df2.name.rlike(\"(?i)^*rose$\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"1004239f-3912-451c-a99f-840762e61f90","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+----------+\n| id|      name|\n+---+----------+\n|  5|Rames rose|\n+---+----------+\n\n+---+------------+\n| id|        name|\n+---+------------+\n|  2|Michael Rose|\n|  4|  Rames Rose|\n|  5|  Rames rose|\n+---+------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#First, the code imports the necessary modules: pyspark and SparkSession from pyspark.sql.\n\n#A SparkSession named spark is created using SparkSession.builder. This session will be used to interact with Spark and perform various operations on the data.\n\n#The code defines the data as data2, which is a list of tuples representing the rows of the DataFrame. Each tuple consists of an ID and a name.\n\n#The spark.createDataFrame() method is called to create a DataFrame named df2 from the provided data and schema. The schema is defined explicitly with column names \"id\" and \"name\".\n\n#Filtering rows using the like method with a string pattern:\n\n#The df2.name.like(\"%rose%\") method filters the DataFrame and selects rows where the \"name\" column contains the substring \"rose\".\n#The % wildcard characters are used to match any sequence of characters before and after the substring \"rose\".\n#The show() method is called to display the filtered DataFrame.\n#Filtering rows using the rlike method with a regular expression pattern:\n\n#The df2.name.rlike(\"(?i)^*rose$\") method filters the DataFrame and selects rows where the \"name\" column matches the provided regular expression pattern.\n#The (?i) flag is used to make the pattern case-insensitive.\n#The ^ character represents the start of the string, * represents zero or more occurrences, and $ represents the end of the string.\n#This filter selects rows where the \"name\" column ends with the substring \"rose\" (case-insensitive).\n#The show() method is called to display the filtered DataFrame.\n#These operations demonstrate how to use the like and rlike methods in PySpark to filter rows based on string patterns and regular expressions, respectively."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"707a33c2-3a4e-4788-a164-ec8c9f917fe9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-filter2.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
