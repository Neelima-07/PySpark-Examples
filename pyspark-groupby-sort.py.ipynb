{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col,sum,avg,max\n\nspark = SparkSession.builder \\\n                    .appName('SparkByExamples.com') \\\n                    .getOrCreate()\n\nsimpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n    (\"Michael\",\"Sales\",\"NV\",86000,56,20000),\n    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n    (\"Raman\",\"Finance\",\"DE\",99000,40,24000),\n    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n    (\"Jeff\",\"Marketing\",\"NV\",80000,25,18000),\n    (\"Kumar\",\"Marketing\",\"NJ\",91000,50,21000)\n  ]\n\nschema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\ndf = spark.createDataFrame(data=simpleData, schema = schema)\ndf.printSchema()\ndf.show(truncate=False)\n\ndf.groupBy(\"state\").sum(\"salary\").show()\n\ndfGroup=df.groupBy(\"state\") \\\n          .agg(sum(\"salary\").alias(\"sum_salary\"))\n          \ndfGroup.show(truncate=False)\n\ndfFilter=dfGroup.filter(dfGroup.sum_salary > 100000)\ndfFilter.show()\n\nfrom pyspark.sql.functions import asc\ndfFilter.sort(\"sum_salary\").show()\n\nfrom pyspark.sql.functions import desc\ndfFilter.sort(desc(\"sum_salary\")).show()\n\ndf.groupBy(\"state\") \\\n  .agg(sum(\"salary\").alias(\"sum_salary\")) \\\n  .filter(col(\"sum_salary\") > 100000)  \\\n  .sort(desc(\"sum_salary\")) \\\n  .show()\n  \ndf.createOrReplaceTempView(\"EMP\")\nspark.sql(\"select state, sum(salary) as sum_salary from EMP \" +\n          \"group by state having sum_salary > 100000 \" + \n          \"order by sum_salary desc\").show()\n\ndf.groupBy(\"state\") \\\n  .sum(\"salary\") \\\n  .withColumnRenamed(\"sum(salary)\", \"sum_salary\") \\\n  .show()\n\ndf.groupBy(\"state\") \\\n  .sum(\"salary\") \\\n  .select(col(\"state\"),col(\"sum(salary)\").alias(\"sum_salary\")) \\\n  .show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"009de43f-641d-4756-88e3-8325382580a7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- state: string (nullable = true)\n |-- salary: long (nullable = true)\n |-- age: long (nullable = true)\n |-- bonus: long (nullable = true)\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|James        |Sales     |NY   |90000 |34 |10000|\n|Michael      |Sales     |NV   |86000 |56 |20000|\n|Robert       |Sales     |CA   |81000 |30 |23000|\n|Maria        |Finance   |CA   |90000 |24 |23000|\n|Raman        |Finance   |DE   |99000 |40 |24000|\n|Scott        |Finance   |NY   |83000 |36 |19000|\n|Jen          |Finance   |NY   |79000 |53 |15000|\n|Jeff         |Marketing |NV   |80000 |25 |18000|\n|Kumar        |Marketing |NJ   |91000 |50 |21000|\n+-------------+----------+-----+------+---+-----+\n\n+-----+-----------+\n|state|sum(salary)|\n+-----+-----------+\n|   NY|     252000|\n|   NV|     166000|\n|   CA|     171000|\n|   DE|      99000|\n|   NJ|      91000|\n+-----+-----------+\n\n+-----+----------+\n|state|sum_salary|\n+-----+----------+\n|NY   |252000    |\n|NV   |166000    |\n|CA   |171000    |\n|DE   |99000     |\n|NJ   |91000     |\n+-----+----------+\n\n+-----+----------+\n|state|sum_salary|\n+-----+----------+\n|   NY|    252000|\n|   NV|    166000|\n|   CA|    171000|\n+-----+----------+\n\n+-----+----------+\n|state|sum_salary|\n+-----+----------+\n|   NV|    166000|\n|   CA|    171000|\n|   NY|    252000|\n+-----+----------+\n\n+-----+----------+\n|state|sum_salary|\n+-----+----------+\n|   NY|    252000|\n|   CA|    171000|\n|   NV|    166000|\n+-----+----------+\n\n+-----+----------+\n|state|sum_salary|\n+-----+----------+\n|   NY|    252000|\n|   CA|    171000|\n|   NV|    166000|\n+-----+----------+\n\n+-----+----------+\n|state|sum_salary|\n+-----+----------+\n|   NY|    252000|\n|   CA|    171000|\n|   NV|    166000|\n+-----+----------+\n\n+-----+----------+\n|state|sum_salary|\n+-----+----------+\n|   NY|    252000|\n|   NV|    166000|\n|   CA|    171000|\n|   DE|     99000|\n|   NJ|     91000|\n+-----+----------+\n\n+-----+----------+\n|state|sum_salary|\n+-----+----------+\n|   NY|    252000|\n|   NV|    166000|\n|   CA|    171000|\n|   DE|     99000|\n|   NJ|     91000|\n+-----+----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#Creating the DataFrame: This section creates a DataFrame df from a list of tuples with specific schema. The DataFrame contains columns such as 'employee_name', 'department', 'state', 'salary', 'age', and 'bonus'.\n\n#Grouping and Aggregating: This section shows how to group the DataFrame by the 'state' column and calculate the sum of 'salary' for each group using the groupBy and sum functions. The results are displayed using show().\n\n#Filtering: This section demonstrates how to filter the grouped DataFrame based on a condition, filtering out groups where the sum of salaries is greater than 100,000.\n\n#Sorting: This section shows how to sort the filtered DataFrame in ascending or descending order based on the sum of salaries using the sort function and asc or desc functions.\n\n#Chaining operations: This section demonstrates how to chain multiple operations together, including filtering, sorting, and selecting columns, using the DataFrame API.\n\n#Using SQL queries: This section shows how to create a temporary view from the DataFrame using createOrReplaceTempView and perform the same grouping, filtering, and sorting using SQL queries with the spark.sql function.\n\n#Renaming columns: This section shows how to rename a column in the result of an aggregation using withColumnRenamed.\n\n#Selecting specific columns: This section demonstrates how to select specific columns from the result of an aggregation using the select function and aliasing the column name using alias.\n\n#These examples showcase the flexibility of PySpark's DataFrame API and SQL queries for performing aggregations, filtering, sorting, and selecting columns on DataFrames."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"01664e35-4635-4539-b056-fa1bc4bfc78f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-groupby-sort.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
