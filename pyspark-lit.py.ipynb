{"cells":[{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\n\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\ndata = [(\"111\",50000),(\"222\",60000),(\"333\",40000)]\ncolumns= [\"EmpId\",\"Salary\"]\ndf = spark.createDataFrame(data = data, schema = columns)\ndf.printSchema()\ndf.show(truncate=False)\n\nfrom pyspark.sql.functions import col,lit\ndf2 = df.select(col(\"EmpId\"),col(\"Salary\"),lit(\"1\").alias(\"lit_value1\"))\ndf2.show(truncate=False)\n\n\nfrom pyspark.sql.functions import when\ndf3 = df2.withColumn(\"lit_value2\", when((col(\"Salary\") >=40000) & (col(\"Salary\") <= 50000),lit(\"100\")).otherwise(lit(\"200\")))\ndf3.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"c52d7201-b34f-49b4-8176-cc93553b22c3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- EmpId: string (nullable = true)\n |-- Salary: long (nullable = true)\n\n+-----+------+\n|EmpId|Salary|\n+-----+------+\n|111  |50000 |\n|222  |60000 |\n|333  |40000 |\n+-----+------+\n\n+-----+------+----------+\n|EmpId|Salary|lit_value1|\n+-----+------+----------+\n|111  |50000 |1         |\n|222  |60000 |1         |\n|333  |40000 |1         |\n+-----+------+----------+\n\n+-----+------+----------+----------+\n|EmpId|Salary|lit_value1|lit_value2|\n+-----+------+----------+----------+\n|111  |50000 |1         |100       |\n|222  |60000 |1         |200       |\n|333  |40000 |1         |100       |\n+-----+------+----------+----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#Creating DataFrame:\n\n#A DataFrame df is created with columns \"EmpId\" and \"Salary\" using the createDataFrame() method. It contains three rows of data.\n#Printing Schema and Data:\n\n#The schema of the DataFrame df is printed using printSchema().\n#The data in the DataFrame df is displayed using show().\n#Selecting Columns and Adding Literal Column:\n\n#The code selects the columns \"EmpId\" and \"Salary\" from the DataFrame df using select() and assigns a literal value of \"1\" to a new column called \"lit_value1\" using the lit() function and alias() method.\n#Displaying the Modified DataFrame:\n\n#The modified DataFrame df2 is displayed using show().\n#Adding Conditional Column:\n\n#The code adds a new column called \"lit_value2\" to the DataFrame df2 using withColumn(). The new column is derived based on a condition using the when() function. If the \"Salary\" is between 40,000 and 50,000 (inclusive), the value is set to \"100\"; otherwise, it is set to \"200\".\n#Displaying the Final DataFrame:\n\n#The final DataFrame df3 is displayed using show().\n#The purpose of this code is to showcase the usage of functions like col(), lit(), and when() in PySpark for manipulating and transforming DataFrames."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a4afccd5-f93e-46f1-9858-e5eccbf4b517","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-lit.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
