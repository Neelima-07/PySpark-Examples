{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder \\\n                    .appName('SparkByExamples.com') \\\n                    .getOrCreate()\n\ndata = [('James','Smith','M',30),\n  ('Anna','Rose','F',41),\n  ('Robert','Williams','M',62), \n]\n\ncolumns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\ndf = spark.createDataFrame(data=data, schema = columns)\ndf.show()\n\nfrom pyspark.sql.functions import concat_ws,col,lit\ndf.select(concat_ws(\",\",df.firstname,df.lastname).alias(\"name\"), \\\n          df.gender,lit(df.salary*2).alias(\"new_salary\")).show()\n\nprint(df.collect())\nrdd=df.rdd.map(lambda x: \n    (x[0]+\",\"+x[1],x[2],x[3]*2)\n    )  \ndf2=rdd.toDF([\"name\",\"gender\",\"new_salary\"]   )\ndf2.show()\n\n\n#Referring Column Names\nrdd2=df.rdd.map(lambda x: \n    (x[\"firstname\"]+\",\"+x[\"lastname\"],x[\"gender\"],x[\"salary\"]*2)\n    ) \n\n\n#Referring Column Names\nrdd2=df.rdd.map(lambda x: \n    (x.firstname+\",\"+x.lastname,x.gender,x.salary*2)\n    ) \n\n\ndef func1(x):\n    firstName=x.firstname\n    lastName=x.lastName\n    name=firstName+\",\"+lastName\n    gender=x.gender.lower()\n    salary=x.salary*2\n    return (name,gender,salary)\n\nrdd2=df.rdd.map(lambda x: func1(x))\n\n#Foeeach example\ndef f(x): print(x)\ndf.rdd.foreach(f)\n\ndf.rdd.foreach(lambda x: \n    print(\"Data ==>\"+x[\"firstname\"]+\",\"+x[\"lastname\"]+\",\"+x[\"gender\"]+\",\"+str(x[\"salary\"]*2))\n    ) \n    \n#Iterate collected data\ndataCollect = df.collect()\nfor row in dataCollect:\n    print(row['firstname'] + \",\" +row['lastname'])\n    \n#Convert to Pandas and Iterate\n\ndataCollect=df.rdd.toLocalIterator()\nfor row in dataCollect:\n    print(row['firstname'] + \",\" +row['lastname'])\n\nimport pandas as pd\npandasDF = df.toPandas()\nfor index, row in pandasDF.iterrows():\n    print(row['firstname'], row['gender'])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"6a8db7f6-5be5-4d0c-bb19-e84239116147","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+--------+------+------+\n|firstname|lastname|gender|salary|\n+---------+--------+------+------+\n|    James|   Smith|     M|    30|\n|     Anna|    Rose|     F|    41|\n|   Robert|Williams|     M|    62|\n+---------+--------+------+------+\n\n+---------------+------+----------+\n|           name|gender|new_salary|\n+---------------+------+----------+\n|    James,Smith|     M|        60|\n|      Anna,Rose|     F|        82|\n|Robert,Williams|     M|       124|\n+---------------+------+----------+\n\n[Row(firstname='James', lastname='Smith', gender='M', salary=30), Row(firstname='Anna', lastname='Rose', gender='F', salary=41), Row(firstname='Robert', lastname='Williams', gender='M', salary=62)]\n+---------------+------+----------+\n|           name|gender|new_salary|\n+---------------+------+----------+\n|    James,Smith|     M|        60|\n|      Anna,Rose|     F|        82|\n|Robert,Williams|     M|       124|\n+---------------+------+----------+\n\nJames,Smith\nAnna,Rose\nRobert,Williams\nJames,Smith\nAnna,Rose\nRobert,Williams\n"]},{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/plain":[],"application/vnd.databricks.v1+bamboolib_hint":"{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}"}},{"output_type":"stream","output_type":"stream","name":"stdout","text":["James M\nAnna F\nRobert M\n"]}],"execution_count":0},{"cell_type":"code","source":["#Creating DataFrame:\n\n#A DataFrame df is created using spark.createDataFrame() with data and schema defined in the code. The DataFrame contains columns \"firstname,\" \"lastname,\" \"gender,\" and \"salary.\"\n#Displaying DataFrame:\n\n#The data in the DataFrame df is displayed using df.show().\n#Selecting Columns and Creating New Columns:\n\n#The code selects the \"firstname\" and \"lastname\" columns from the DataFrame df using select() and concatenates them using concat_ws() to create a new column called \"name.\"\n#The \"gender\" column is directly selected, and a new column called \"new_salary\" is created by multiplying the \"salary\" column by 2 using lit().\n#Displaying Modified DataFrame:\n\n#The modified DataFrame is displayed using show().\n#Converting DataFrame to RDD and Creating a New DataFrame:\n\n#The DataFrame df is converted to an RDD using rdd() and mapped to a new RDD rdd by applying a lambda function that transforms the columns.\n#The RDD rdd is converted back to a DataFrame df2 using toDF() and column names are provided.\n#Using Column Names to Refer:\n\n#The code provides alternative approaches to access columns by name, either using x[\"column_name\"] or directly using x.column_name.\n#Defining a Function and Applying on RDD:\n\n#The code defines a function func1() that takes a row as input, extracts the required columns, performs transformations, and returns a tuple.\n#The RDD is mapped using the func1() function to create a new RDD rdd2.\n#Performing Actions on RDD:\n\n#Examples are provided to showcase different actions on RDDs, such as foreach() and collect().\n#foreach() is used to apply a function on each element of the RDD.\n#collect() is used to retrieve all the data from the RDD and iterate over it.\n#Converting DataFrame to Pandas and Iterating:\n\n#The DataFrame df is converted to a Pandas DataFrame pandasDF using toPandas().\n#The Pandas DataFrame is iterated using iterrows() to access each row and its corresponding columns.\n#The purpose of this code is to demonstrate various operations and transformations that can be performed on a PySpark DataFrame, including selecting columns, creating new columns, converting to RDD, applying functions, performing actions, and converting to a Pandas DataFrame for further operations."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a31f12cb-677f-427e-bd6c-8d3457970122","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-loop.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
