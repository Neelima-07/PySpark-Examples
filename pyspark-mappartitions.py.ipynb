{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\ndata = [('James','Smith','M',3000),\n  ('Anna','Rose','F',4100),\n  ('Robert','Williams','M',6200), \n]\n\ncolumns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\ndf = spark.createDataFrame(data=data, schema = columns)\ndf.show()\n\n#Example 1 mapPartitions()\ndef reformat(partitionData):\n    for row in partitionData:\n        yield [row.firstname+\",\"+row.lastname,row.salary*10/100]\ndf.rdd.mapPartitions(reformat).toDF().show()\n\n#Example 2 mapPartitions()\ndef reformat2(partitionData):\n  updatedData = []\n  for row in partitionData:\n    name=row.firstname+\",\"+row.lastname\n    bonus=row.salary*10/100\n    updatedData.append([name,bonus])\n  return iter(updatedData)\n\ndf2=df.rdd.mapPartitions(reformat2).toDF([\"name\",\"bonus\"])\ndf2.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"46cfc46a-e0ef-4d82-909f-3a8a07ee16e8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+--------+------+------+\n|firstname|lastname|gender|salary|\n+---------+--------+------+------+\n|    James|   Smith|     M|  3000|\n|     Anna|    Rose|     F|  4100|\n|   Robert|Williams|     M|  6200|\n+---------+--------+------+------+\n\n+---------------+-----+\n|             _1|   _2|\n+---------------+-----+\n|    James,Smith|300.0|\n|      Anna,Rose|410.0|\n|Robert,Williams|620.0|\n+---------------+-----+\n\n+---------------+-----+\n|           name|bonus|\n+---------------+-----+\n|    James,Smith|300.0|\n|      Anna,Rose|410.0|\n|Robert,Williams|620.0|\n+---------------+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#Importing Libraries:\n\n#The necessary library, pyspark.sql.SparkSession, is imported to create a SparkSession and work with DataFrames.\n#Creating SparkSession:\n\n#spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate() creates a SparkSession with the application name \"SparkByExamples.com\".\n#Creating DataFrame:\n\n#The code defines a list of tuples data and a list of column names columns to represent the data and schema, respectively.\n#df = spark.createDataFrame(data=data, schema=columns) creates a DataFrame df using the provided data and schema.\n#Displaying DataFrame:\n\n#df.show() is used to display the data in the DataFrame df.\n#Example 1: mapPartitions() with yield:\n\n#The code defines a function reformat() that takes a partition of data as input.\n#The function iterates over each row in the partition, concatenates the \"firstname\" and \"lastname\" columns, and calculates the bonus amount based on the \"salary\" column.\n#The yield keyword is used to emit each modified row as a list.\n#df.rdd.mapPartitions(reformat).toDF().show() applies the mapPartitions() transformation on the RDD of df using the reformat() function, converts the resulting RDD back to a DataFrame using toDF(), and displays the output.\n#Example 2: mapPartitions() with list accumulation:\n\n#The code defines a function reformat2() that takes a partition of data as input.\n#The function initializes an empty list, updatedData.\n#The function iterates over each row in the partition, concatenates the \"firstname\" and \"lastname\" columns, and calculates the bonus amount based on the \"salary\" column.\n#The concatenated name and bonus amount are appended as a list to updatedData.\n#The updated list updatedData is returned as an iterator using iter().\n#df.rdd.mapPartitions(reformat2).toDF(\"name\", \"bonus\").show() applies the mapPartitions() transformation on the RDD of df using the reformat2() function, converts the resulting RDD back to a DataFrame with specified column names using toDF(), and displays the output.\n#Both examples illustrate how to use mapPartitions() to apply custom transformations on each partition of an RDD in PySpark."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4c714bbc-8295-4526-9005-26bc4b34a25b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d5295f15-7d91-4178-8d5b-d17a5860e50d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-mappartitions.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
