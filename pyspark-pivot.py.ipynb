{"cells":[{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import expr\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ndata = [(\"Banana\",1000,\"USA\"), (\"Carrots\",1500,\"USA\"), (\"Beans\",1600,\"USA\"), \\\n      (\"Orange\",2000,\"USA\"),(\"Orange\",2000,\"USA\"),(\"Banana\",400,\"China\"), \\\n      (\"Carrots\",1200,\"China\"),(\"Beans\",1500,\"China\"),(\"Orange\",4000,\"China\"), \\\n      (\"Banana\",2000,\"Canada\"),(\"Carrots\",2000,\"Canada\"),(\"Beans\",2000,\"Mexico\")]\n\ncolumns= [\"Product\",\"Amount\",\"Country\"]\ndf = spark.createDataFrame(data = data, schema = columns)\ndf.printSchema()\ndf.show(truncate=False)\n\npivotDF = df.groupBy(\"Product\").pivot(\"Country\").sum(\"Amount\")\npivotDF.printSchema()\npivotDF.show(truncate=False)\n\npivotDF = df.groupBy(\"Product\",\"Country\") \\\n      .sum(\"Amount\") \\\n      .groupBy(\"Product\") \\\n      .pivot(\"Country\") \\\n      .sum(\"sum(Amount)\")\npivotDF.printSchema()\npivotDF.show(truncate=False)\n\n\n\"\"\" unpivot \"\"\"\n\"\"\" unpivot \"\"\"\nunpivotExpr = \"stack(3, 'Canada', Canada, 'China', China, 'Mexico', Mexico) as (Country,Total)\"\nunPivotDF = pivotDF.select(\"Product\", expr(unpivotExpr)) \\\n    .where(\"Total is not null\")\nunPivotDF.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"911d4d6e-7328-45f3-a0ef-52f3615a2695","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- Product: string (nullable = true)\n |-- Amount: long (nullable = true)\n |-- Country: string (nullable = true)\n\n+-------+------+-------+\n|Product|Amount|Country|\n+-------+------+-------+\n|Banana |1000  |USA    |\n|Carrots|1500  |USA    |\n|Beans  |1600  |USA    |\n|Orange |2000  |USA    |\n|Orange |2000  |USA    |\n|Banana |400   |China  |\n|Carrots|1200  |China  |\n|Beans  |1500  |China  |\n|Orange |4000  |China  |\n|Banana |2000  |Canada |\n|Carrots|2000  |Canada |\n|Beans  |2000  |Mexico |\n+-------+------+-------+\n\nroot\n |-- Product: string (nullable = true)\n |-- Canada: long (nullable = true)\n |-- China: long (nullable = true)\n |-- Mexico: long (nullable = true)\n |-- USA: long (nullable = true)\n\n+-------+------+-----+------+----+\n|Product|Canada|China|Mexico|USA |\n+-------+------+-----+------+----+\n|Orange |null  |4000 |null  |4000|\n|Beans  |null  |1500 |2000  |1600|\n|Banana |2000  |400  |null  |1000|\n|Carrots|2000  |1200 |null  |1500|\n+-------+------+-----+------+----+\n\nroot\n |-- Product: string (nullable = true)\n |-- Canada: long (nullable = true)\n |-- China: long (nullable = true)\n |-- Mexico: long (nullable = true)\n |-- USA: long (nullable = true)\n\n+-------+------+-----+------+----+\n|Product|Canada|China|Mexico|USA |\n+-------+------+-----+------+----+\n|Orange |null  |4000 |null  |4000|\n|Beans  |null  |1500 |2000  |1600|\n|Banana |2000  |400  |null  |1000|\n|Carrots|2000  |1200 |null  |1500|\n+-------+------+-----+------+----+\n\n+-------+-------+-----+\n|Product|Country|Total|\n+-------+-------+-----+\n|Orange |China  |4000 |\n|Beans  |China  |1500 |\n|Beans  |Mexico |2000 |\n|Banana |Canada |2000 |\n|Banana |China  |400  |\n|Carrots|Canada |2000 |\n|Carrots|China  |1200 |\n+-------+-------+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#Importing Libraries: The code imports the necessary libraries: pyspark, SparkSession, and expr from pyspark.sql.functions.\n\n#Creating SparkSession: A Spark session is created using SparkSession.builder.appName('SparkByExamples.com').getOrCreate().\n\n#Creating DataFrame: A DataFrame named df is created from a list of tuples data and column names columns using spark.createDataFrame(data=data, schema=columns).\n\n#Displaying DataFrame Schema and Content: The schema and content of the DataFrame df are displayed using df.printSchema() and df.show(truncate=False).\n\n#Pivot Operation: The DataFrame df is pivoted using the groupBy(\"Product\").pivot(\"Country\").sum(\"Amount\") operation. This groups the data by \"Product\", creates columns based on distinct \"Country\" values, and calculates the sum of \"Amount\" for each combination.\n\n#Displaying Pivot DataFrame Schema and Content: The schema and content of the pivot DataFrame pivotDF are displayed using pivotDF.printSchema() and pivotDF.show(truncate=False).\n\n#Chained Pivot Operation: The DataFrame df is further transformed using chained operations. It performs grouping, summing, grouping again, and pivoting to calculate the sum of \"Amount\" for each \"Product\" and \"Country\" combination.\n\n#Displaying Chained Pivot DataFrame Schema and Content: The schema and content of the chained pivot DataFrame pivotDF are displayed using pivotDF.printSchema() and pivotDF.show(truncate=False).\n\n#Unpivot Operation: The pivot DataFrame pivotDF is unpivoted using stack() and expr() functions. The stack() function is used to unpivot the data by specifying the number of columns and their corresponding aliases. The resulting DataFrame is named unPivotDF.\n\n#Displaying Unpivot DataFrame: The unpivot DataFrame unPivotDF is displayed using unPivotDF.show(truncate=False).\n\n#The code demonstrates how to perform pivot and unpivot operations in PySpark to reshape and transform data between wide and long formats.\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b8fe0727-6f61-4ff7-9b73-5894e82315e7","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-pivot.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
