{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ndept = [(\"Finance\",10), \\\n    (\"Marketing\",20), \\\n    (\"Sales\",30), \\\n    (\"IT\",40) \\\n  ]\n\nrdd=spark.sparkContext.parallelize(dept)\nprint(rdd)\ndataColl=rdd.collect()\n\nfor row in dataColl:\n    print(row[0] + \",\" +str(row[1]))\n\"\"\"\ndeptColumns = [\"dept_name\",\"dept_id\"]\ndeptDF = spark.createDataFrame(data=dept, schema = deptColumns)\ndeptDF.printSchema()\ndeptDF.show(truncate=False)\n\ndataCollect = deptDF.collect()\n\nprint(dataCollect)\n\ndataCollect2 = deptDF.select(\"dept_name\").collect()\nprint(dataCollect2)\n\nfor row in dataCollect:\n    print(row['dept_name'] + \",\" +str(row['dept_id']))\n\"\"\""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"de74d828-49c6-4bc7-8d12-eb5f41d217c9","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["ParallelCollectionRDD[53] at readRDDFromInputStream at PythonRDD.scala:435\nFinance,10\nMarketing,20\nSales,30\nIT,40\nOut[1]: '\\ndeptColumns = [\"dept_name\",\"dept_id\"]\\ndeptDF = spark.createDataFrame(data=dept, schema = deptColumns)\\ndeptDF.printSchema()\\ndeptDF.show(truncate=False)\\n\\ndataCollect = deptDF.collect()\\n\\nprint(dataCollect)\\n\\ndataCollect2 = deptDF.select(\"dept_name\").collect()\\nprint(dataCollect2)\\n\\nfor row in dataCollect:\\n    print(row[\\'dept_name\\'] + \",\" +str(row[\\'dept_id\\']))\\n'"]}],"execution_count":0},{"cell_type":"code","source":["#Importing Libraries: The code imports the necessary library, SparkSession from pyspark.sql.\n\n#Creating SparkSession: A Spark session is created using SparkSession.builder.appName('SparkByExamples.com').getOrCreate().\n\n#Creating RDD: A list of tuples named dept is created. The RDD rdd is created by parallelizing the dept list using spark.sparkContext.parallelize(dept).\n\n#Printing RDD: The RDD rdd is printed using print(rdd). This will show information about the RDD, such as its type and storage details.\n\n#Collecting and Printing RDD Elements: The RDD rdd is collected into the dataColl list using rdd.collect(). The elements of the RDD are then printed using a loop, where each element is accessed as row[0] and row[1].\n\n#Creating DataFrame: The commented code block shows an alternative way to create a DataFrame deptDF using spark.createDataFrame(data=dept, schema=deptColumns). It defines the column names as deptColumns and uses the dept list to create the DataFrame.\n\n#Printing DataFrame Schema and Content: The commented code block shows how to print the schema and content of the DataFrame deptDF using deptDF.printSchema() and deptDF.show(truncate=False).\n\n#Collecting DataFrame Elements: The commented code block demonstrates how to collect the elements of the DataFrame into a list using deptDF.collect(). The resulting list is stored in dataCollect.\n\n#Printing Collected DataFrame Elements: The commented code block shows how to print the collected DataFrame elements. It uses a loop where each row is accessed using column names, such as row['dept_name'] and row['dept_id'].\n\n#The code demonstrates how to create an RDD, collect and print its elements, and how to create a DataFrame using either an RDD or by explicitly defining a schema."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a2ffc3b5-f027-4838-a9c0-743d7adba773","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-print-contents.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
