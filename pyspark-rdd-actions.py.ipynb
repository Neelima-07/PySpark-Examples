{"cells":[{"cell_type":"code","source":["\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\ndata=[(\"Z\", 1),(\"A\", 20),(\"B\", 30),(\"C\", 40),(\"B\", 30),(\"B\", 60)]\ninputRDD = spark.sparkContext.parallelize(data)\n  \nlistRdd = spark.sparkContext.parallelize([1,2,3,4,5,3,2])\n\n#aggregate\nseqOp = (lambda x, y: x + y)\ncombOp = (lambda x, y: x + y)\nagg=listRdd.aggregate(0, seqOp, combOp)\nprint(agg) # output 20\n\n#aggregate 2\nseqOp2 = (lambda x, y: (x[0] + y, x[1] + 1))\ncombOp2 = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\nagg2=listRdd.aggregate((0, 0), seqOp2, combOp2)\nprint(agg2) # output (20,7)\n\nagg2=listRdd.treeAggregate(0,seqOp, combOp)\nprint(agg2) # output 20\n\n#fold\nfrom operator import add\nfoldRes=listRdd.fold(0, add)\nprint(foldRes) # output 20\n\n#reduce\nredRes=listRdd.reduce(add)\nprint(redRes) # output 20\n\n#treeReduce. This is similar to reduce\nadd = lambda x, y: x + y\nredRes=listRdd.treeReduce(add)\nprint(redRes) # output 20\n\n#Collect\ndata = listRdd.collect()\nprint(data)\n\n#count, countApprox, countApproxDistinct\nprint(\"Count : \"+str(listRdd.count()))\n#Output: Count : 20\nprint(\"countApprox : \"+str(listRdd.countApprox(1200)))\n#Output: countApprox : (final: [7.000, 7.000])\nprint(\"countApproxDistinct : \"+str(listRdd.countApproxDistinct()))\n#Output: countApproxDistinct : 5\nprint(\"countApproxDistinct : \"+str(inputRDD.countApproxDistinct()))\n#Output: countApproxDistinct : 5\n\n#countByValue, countByValueApprox\nprint(\"countByValue :  \"+str(listRdd.countByValue()))\n\n\n#first\nprint(\"first :  \"+str(listRdd.first()))\n#Output: first :  1\nprint(\"first :  \"+str(inputRDD.first()))\n#Output: first :  (Z,1)\n\n#top\nprint(\"top : \"+str(listRdd.top(2)))\n#Output: take : 5,4\nprint(\"top : \"+str(inputRDD.top(2)))\n#Output: take : (Z,1),(C,40)\n\n#min\nprint(\"min :  \"+str(listRdd.min()))\n#Output: min :  1\nprint(\"min :  \"+str(inputRDD.min()))\n#Output: min :  (A,20)  \n\n#max\nprint(\"max :  \"+str(listRdd.max()))\n#Output: max :  5\nprint(\"max :  \"+str(inputRDD.max()))\n#Output: max :  (Z,1)\n\n#take, takeOrdered, takeSample\nprint(\"take : \"+ str(listRdd.take(2)))\n#Output: take : 1,2\nprint(\"takeOrdered : \"+ str(listRdd.takeOrdered(2)))\n#Output: takeOrdered : 1,2\nprint(\"takeSample: \" + str(listRdd.takeSample(num=2, withReplacement=False)))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"57aa8da2-6a2a-4003-b635-644f027e52d5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["20\n(20, 7)\n20\n20\n20\n20\n[1, 2, 3, 4, 5, 3, 2]\nCount : 7\ncountApprox : 7\ncountApproxDistinct : 5\ncountApproxDistinct : 5\ncountByValue :  defaultdict(<class 'int'>, {1: 1, 2: 2, 3: 2, 4: 1, 5: 1})\nfirst :  1\nfirst :  ('Z', 1)\ntop : [5, 4]\ntop : [('Z', 1), ('C', 40)]\nmin :  1\nmin :  ('A', 20)\nmax :  5\nmax :  ('Z', 1)\ntake : [1, 2]\ntakeOrdered : [1, 2]\ntakeSample: [3, 1]\n"]}],"execution_count":0},{"cell_type":"code","source":["#Importing Libraries: The code imports the necessary library, SparkSession, from pyspark.sql.\n\n#Creating SparkSession: A Spark session is created using SparkSession.builder.appName('SparkByExamples.com').getOrCreate().\n\n#Creating RDD: An RDD named inputRDD is created from a list of tuples data using spark.sparkContext.parallelize(data).\n\n#Aggregate: The RDD listRdd is aggregated using the aggregate method with a sequence function (seqOp) and a combination function (combOp). The result is stored in agg and printed.\n\n#Aggregate 2: Another aggregation is performed on listRdd using a different set of sequence and combination functions (seqOp2 and combOp2). The result is stored in agg2 and printed.\n\n#Tree Aggregate: The RDD listRdd is tree-aggregated using the treeAggregate method with the same sequence and combination functions. The result is stored in agg2 and printed.\n\n#Fold: The RDD listRdd is folded using the fold method with the add function. The result is stored in foldRes and printed.\n\n#Reduce: The RDD listRdd is reduced using the reduce method with the add function. The result is stored in redRes and printed.\n\n#Tree Reduce: The RDD listRdd is tree-reduced using the treeReduce method with the add function. The result is stored in redRes and printed.\n\n#Collect: The RDD listRdd is collected into a Python list using the collect method, and the list is printed.\n\n#Count: The count of elements in the RDD listRdd is printed using the count method.\n\n#Count Approximation: An approximate count of elements in the RDD listRdd is printed using the countApprox method.\n\n#Count Approximate Distinct: An approximate count of distinct elements in the RDD listRdd is printed using the countApproxDistinct method.\n\n#Count Approximate Distinct (for inputRDD): An approximate count of distinct elements in the RDD inputRDD is printed using the countApproxDistinct method.\n\n#Count By Value: The count of each unique value in the RDD listRdd is printed using the countByValue method.\n\n#First: The first element of the RDD listRdd is printed using the first method.\n\n#Top: The top 2 elements of the RDD listRdd are printed using the top method.\n\n#Min: The minimum value in the RDD listRdd is printed using the min method.\n\n#Max: The maximum value in the RDD listRdd is printed using the max method.\n\n#Take: The first 2 elements of the RDD listRdd are printed using the take method.\n\n#Take Ordered: The first 2 elements of the RDD listRdd in ascending order are printed using the takeOrdered method.\n\n#Take Sample: A random sample of elements from the RDD listRdd is printed using the takeSample method.\n\n#The code demonstrates various actions that can be performed on RDDs in PySpark, including aggregation, reduction, counting, retrieving elements, finding min/max values, and\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"56047257-468b-4087-82ec-0e0b713ed198","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"059dac24-6d26-4665-a1b4-73583acadd1f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-rdd-actions.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
