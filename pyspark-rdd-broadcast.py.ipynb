{"cells":[{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\nstates = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\nbroadcastStates = spark.sparkContext.broadcast(states)\n\ndata = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n  ]\n\nrdd = spark.sparkContext.parallelize(data)\n\ndef state_convert(code):\n    return broadcastStates.value[code]\n\nresult = rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).collect()\nprint(result)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"8de13497-7864-4a86-8e5a-3849b29e1de3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["[('James', 'Smith', 'USA', 'California'), ('Michael', 'Rose', 'USA', 'New York'), ('Robert', 'Williams', 'USA', 'California'), ('Maria', 'Jones', 'USA', 'Florida')]\n"]}],"execution_count":0},{"cell_type":"code","source":["#Importing Libraries: The code imports the necessary library, pyspark and SparkSession from pyspark.sql.\n\n#Creating SparkSession: A Spark session is created using SparkSession.builder.appName('SparkByExamples.com').getOrCreate().\n\n#Creating a Dictionary: A dictionary named states is created, which maps state codes to their corresponding names.\n\n#Broadcasting States: The dictionary states is broadcasted using spark.sparkContext.broadcast(states). This allows efficient sharing of the data across all worker nodes.\n\n#Creating RDD: An RDD named rdd is created from a list of tuples data using spark.sparkContext.parallelize(data).\n\n#Defining state_convert Function: A function named state_convert is defined, which takes a state code as input and returns the corresponding state name using the broadcasted dictionary broadcastStates.\n\n#Mapping and Collecting: The RDD rdd is mapped using a lambda function that applies the state_convert function to convert the state code to its name. The result is collected using the collect action and stored in the result variable.\n\n#Printing the Result: The result list is printed, which contains tuples with the original data along with the converted state names.\n\n#The code demonstrates how broadcasting can be used to efficiently distribute and utilize a shared read-only variable (states dictionary) across all the worker nodes in a Spark cluster."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0b1554f0-50c8-4d42-af67-ed8b481ac6ff","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-rdd-broadcast.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
