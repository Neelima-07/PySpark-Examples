{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ndata = [\"Project\",\n\"Gutenberg’s\",\n\"Alice’s\",\n\"Adventures\",\n\"in\",\n\"Wonderland\",\n\"Project\",\n\"Gutenberg’s\",\n\"Adventures\",\n\"in\",\n\"Wonderland\",\n\"Project\",\n\"Gutenberg’s\"]\n\nrdd=spark.sparkContext.parallelize(data)\n\nrdd2=rdd.map(lambda x: (x,1))\nfor element in rdd2.collect():\n    print(element)\n    \ndata = [('James','Smith','M',30),\n  ('Anna','Rose','F',41),\n  ('Robert','Williams','M',62), \n]\n\ncolumns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\ndf = spark.createDataFrame(data=data, schema = columns)\ndf.show()\n\nrdd2=df.rdd.map(lambda x: \n    (x[0]+\",\"+x[1],x[2],x[3]*2)\n    )  \ndf2=rdd2.toDF([\"name\",\"gender\",\"new_salary\"]   )\ndf2.show()\n\n\n#Referring Column Names\nrdd2=df.rdd.map(lambda x: \n    (x[\"firstname\"]+\",\"+x[\"lastname\"],x[\"gender\"],x[\"salary\"]*2)\n    ) \n\n\n#Referring Column Names\nrdd2=df.rdd.map(lambda x: \n    (x.firstname+\",\"+x.lastname,x.gender,x.salary*2)\n    ) \n\n\ndef func1(x):\n    firstName=x.firstname\n    lastName=x.lastname\n    name=firstName+\",\"+lastName\n    gender=x.gender.lower()\n    salary=x.salary*2\n    return (name,gender,salary)\n\nrdd2=df.rdd.map(lambda x: func1(x)).toDF().show()\nrdd2=df.rdd.map(func1).toDF().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"2bdfa8a4-a05f-4370-aa41-63586061cc21","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["('Project', 1)\n('Gutenberg’s', 1)\n('Alice’s', 1)\n('Adventures', 1)\n('in', 1)\n('Wonderland', 1)\n('Project', 1)\n('Gutenberg’s', 1)\n('Adventures', 1)\n('in', 1)\n('Wonderland', 1)\n('Project', 1)\n('Gutenberg’s', 1)\n+---------+--------+------+------+\n|firstname|lastname|gender|salary|\n+---------+--------+------+------+\n|    James|   Smith|     M|    30|\n|     Anna|    Rose|     F|    41|\n|   Robert|Williams|     M|    62|\n+---------+--------+------+------+\n\n+---------------+------+----------+\n|           name|gender|new_salary|\n+---------------+------+----------+\n|    James,Smith|     M|        60|\n|      Anna,Rose|     F|        82|\n|Robert,Williams|     M|       124|\n+---------------+------+----------+\n\n+---------------+---+---+\n|             _1| _2| _3|\n+---------------+---+---+\n|    James,Smith|  m| 60|\n|      Anna,Rose|  f| 82|\n|Robert,Williams|  m|124|\n+---------------+---+---+\n\n+---------------+---+---+\n|             _1| _2| _3|\n+---------------+---+---+\n|    James,Smith|  m| 60|\n|      Anna,Rose|  f| 82|\n|Robert,Williams|  m|124|\n+---------------+---+---+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#Importing SparkSession: The SparkSession class is imported from the pyspark.sql module. It is the entry point to programming Spark with the DataFrame and SQL API.\n\n#Creating a SparkSession: The SparkSession is created using the SparkSession.builder method with the application name set to 'SparkByExamples.com'. If a SparkSession already exists, it returns that instance; otherwise, it creates a new one.\n\n#Creating an RDD: An RDD named rdd is created using spark.sparkContext.parallelize(data), where data is a list of strings.\n\n#Mapping RDD elements: The RDD elements are transformed using rdd.map(lambda x: (x,1)). Each string in the RDD is transformed into a tuple with the string itself as the key and the value 1.\n\n#Collecting and printing RDD elements: The transformed RDD elements are collected using rdd2.collect(). It returns a list of key-value tuples, and each tuple is printed.\n\n#Creating a DataFrame: A DataFrame named df is created using spark.createDataFrame(data=data, schema=columns). The data is a list of tuples, and columns is a list of column names.\n\n#Displaying DataFrame: The df.show() method is used to display the contents of the DataFrame.\n\n#Transforming DataFrame to RDD: The DataFrame df is transformed into an RDD named rdd2 using df.rdd.map(lambda x: (x[0]+\",\"+x[1],x[2],x[3]*2)).\n\n#Converting RDD back to DataFrame: The RDD rdd2 is converted back to a DataFrame named df2 using rdd2.toDF([\"name\",\"gender\",\"new_salary\"]).\n\n#Displaying transformed DataFrame: The df2.show() method is used to display the contents of the transformed DataFrame.\n\n#Referring column names in RDD transformation: Two examples are provided where the RDD transformation is done by referring to column names instead of indices: x[\"firstname\"]+\",\"+x[\"lastname\"] and x.firstname+\",\"+x.lastname.\n\n#Defining a function for RDD transformation: The function func1 is defined to transform each element of the DataFrame RDD. It extracts the required columns, performs some calculations, and returns a tuple (name, gender, salary). Two examples are given where the RDD transformation is done using this function: df.rdd.map(lambda x: func1(x)).toDF().show() and df.rdd.map(func1).toDF().show().\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f75bbdec-06e5-4638-9f50-4003390335dc","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-rdd-map.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
