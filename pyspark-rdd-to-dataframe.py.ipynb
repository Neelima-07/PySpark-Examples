{"cells":[{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ndept = [(\"Finance\",10), \n        (\"Marketing\",20), \n        (\"Sales\",30), \n        (\"IT\",40) \n      ]\nrdd = spark.sparkContext.parallelize(dept)\n\ndf = rdd.toDF()\ndf.printSchema()\ndf.show(truncate=False)\n\ndeptColumns = [\"dept_name\",\"dept_id\"]\ndf2 = rdd.toDF(deptColumns)\ndf2.printSchema()\ndf2.show(truncate=False)\n\ndeptDF = spark.createDataFrame(data=dept, schema = deptColumns)\ndeptDF.printSchema()\ndeptDF.show(truncate=False)\n\n\nfrom pyspark.sql.types import StructType,StructField, StringType\ndeptSchema = StructType([       \n    StructField('dept_name', StringType(), True),\n    StructField('dept_id', StringType(), True)\n])\n\ndeptDF1 = spark.createDataFrame(data=dept, schema = deptSchema)\ndeptDF1.printSchema()\ndeptDF1.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"53a96012-13a3-4876-85cb-f889ece4b022","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- _1: string (nullable = true)\n |-- _2: long (nullable = true)\n\n+---------+---+\n|_1       |_2 |\n+---------+---+\n|Finance  |10 |\n|Marketing|20 |\n|Sales    |30 |\n|IT       |40 |\n+---------+---+\n\nroot\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|Finance  |10     |\n|Marketing|20     |\n|Sales    |30     |\n|IT       |40     |\n+---------+-------+\n\nroot\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|Finance  |10     |\n|Marketing|20     |\n|Sales    |30     |\n|IT       |40     |\n+---------+-------+\n\nroot\n |-- dept_name: string (nullable = true)\n |-- dept_id: string (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|Finance  |10     |\n|Marketing|20     |\n|Sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#Importing necessary modules: The pyspark module and the SparkSession class are imported from the pyspark.sql module.\n\n#Creating a SparkSession: The SparkSession is created using the SparkSession.builder method with the application name set to 'SparkByExamples.com'. If a SparkSession already exists, it returns that instance; otherwise, it creates a new one.\n\n#Creating data: The dept variable is defined as a list of tuples. Each tuple represents a department and consists of a department name and a department ID.\n\n#Creating an RDD: An RDD named rdd is created using spark.sparkContext.parallelize(dept). The RDD is parallelized from the dept list.\n\n#Converting RDD to DataFrame: The RDD is converted to a DataFrame named df using rdd.toDF(). Since the RDD does not have a specified schema, Spark infers the schema from the data.\n\n#Printing DataFrame schema and contents: The df.printSchema() method is used to print the schema of the DataFrame. The df.show(truncate=False) method is used to display the contents of the DataFrame without truncating the output.\n\n#Creating a DataFrame with specified column names: Another DataFrame named df2 is created from the RDD using rdd.toDF(deptColumns), where deptColumns is a list containing the column names.\n\n#Printing DataFrame schema and contents: The schema and contents of df2 are printed using the same methods as before.\n\n#Creating a DataFrame with specified schema: The dept list and deptColumns are used to create a DataFrame named deptDF explicitly specifying the schema using spark.createDataFrame(data=dept, schema=deptColumns).\n\n#Printing DataFrame schema and contents: The schema and contents of deptDF are printed using the same methods as before.\n\n#Defining a schema with StructType: The StructType class from the pyspark.sql.types module is imported. A deptSchema is defined using StructType and StructField to specify the schema structure.\n\n#Creating a DataFrame with the custom schema: The dept list and deptSchema are used to create a DataFrame named deptDF1 with the specified schema using spark.createDataFrame(data=dept, schema=deptSchema).\n\n#Printing DataFrame schema and contents: The schema and contents of deptDF1 are printed using the same methods as before."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"31050760-43a6-4827-9644-ffe68927e53b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-rdd-to-dataframe.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
