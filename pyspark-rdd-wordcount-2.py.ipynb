{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\ndata = [\"Project Gutenberg’s\",\n        \"Alice’s Adventures in Wonderland\",\n        \"Project Gutenberg’s\",\n        \"Adventures in Wonderland\",\n        \"Project Gutenberg’s\"]\nrdd=spark.sparkContext.parallelize(data)\n\nfor element in rdd.collect():\n    print(element)\n\n#Flatmap    \nrdd2=rdd.flatMap(lambda x: x.split(\" \"))\nfor element in rdd2.collect():\n    print(element)\n#map\nrdd3=rdd2.map(lambda x: (x,1))\nfor element in rdd3.collect():\n    print(element)\n#reduceByKey\nrdd4=rdd3.reduceByKey(lambda a,b: a+b)\nfor element in rdd4.collect():\n    print(element)\n#map\nrdd5 = rdd4.map(lambda x: (x[1],x[0])).sortByKey()\nfor element in rdd5.collect():\n    print(element)\n#filter\nrdd6 = rdd5.filter(lambda x : 'a' in x[1])\nfor element in rdd6.collect():\n    print(element)\n\nfrom pyspark.sql.functions import col,expr\ndata=[(\"2019-01-23\",1),(\"2019-06-24\",2),(\"2019-09-20\",3)]\nspark.createDataFrame(data).toDF(\"date\",\"increment\") \\\n    .select(col(\"date\"),col(\"increment\"), \\\n      expr(\"add_months(to_date(date,'yyyy-MM-dd'),cast(increment as int))\").alias(\"inc_date\")) \\\n    .show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"507db923-b00b-4371-b0c8-047f9d1deb8a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Project Gutenberg’s\nAlice’s Adventures in Wonderland\nProject Gutenberg’s\nAdventures in Wonderland\nProject Gutenberg’s\nProject\nGutenberg’s\nAlice’s\nAdventures\nin\nWonderland\nProject\nGutenberg’s\nAdventures\nin\nWonderland\nProject\nGutenberg’s\n('Project', 1)\n('Gutenberg’s', 1)\n('Alice’s', 1)\n('Adventures', 1)\n('in', 1)\n('Wonderland', 1)\n('Project', 1)\n('Gutenberg’s', 1)\n('Adventures', 1)\n('in', 1)\n('Wonderland', 1)\n('Project', 1)\n('Gutenberg’s', 1)\n('Gutenberg’s', 3)\n('Adventures', 2)\n('Wonderland', 2)\n('Alice’s', 1)\n('in', 2)\n('Project', 3)\n(1, 'Alice’s')\n(2, 'Adventures')\n(2, 'Wonderland')\n(2, 'in')\n(3, 'Gutenberg’s')\n(3, 'Project')\n(2, 'Wonderland')\n+----------+---------+----------+\n|      date|increment|  inc_date|\n+----------+---------+----------+\n|2019-01-23|        1|2019-02-23|\n|2019-06-24|        2|2019-08-24|\n|2019-09-20|        3|2019-12-20|\n+----------+---------+----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#Importing necessary modules: The SparkSession class is imported from the pyspark.sql module.\n\n#Creating a SparkSession: The SparkSession is created using the SparkSession.builder method with the application name set to 'SparkByExamples.com'. If a SparkSession already exists, it returns that instance; otherwise, it creates a new one.\n\n#Creating data: The data variable is defined as a list of strings.\n\n#Creating an RDD: An RDD named rdd is created using spark.sparkContext.parallelize(data). The RDD is parallelized from the data list.\n\n#Collecting and printing RDD elements: The RDD elements are collected using rdd.collect(), and each element is printed.\n\n#Applying flatMap transformation: The flatMap transformation is applied to rdd using rdd.flatMap(lambda x: x.split(\" \")). It splits each string into words and creates a new RDD with all the words flattened.\n\n#Collecting and printing RDD elements: The transformed RDD elements are collected using rdd2.collect(), and each element is printed.\n\n#Applying map transformation: The map transformation is applied to rdd2 using rdd2.map(lambda x: (x,1)). It assigns a count of 1 to each word, creating a key-value pair RDD.\n\n#Collecting and printing RDD elements: The transformed RDD elements are collected using rdd3.collect(), and each element is printed.\n\n#Applying reduceByKey transformation: The reduceByKey transformation is applied to rdd3 using rdd3.reduceByKey(lambda a,b: a+b). It reduces the RDD by summing up the counts for each word.\n\n#Collecting and printing RDD elements: The transformed RDD elements are collected using rdd4.collect(), and each element is printed.\n\n#Applying map transformation: The map transformation is applied to rdd4 using rdd4.map(lambda x: (x[1],x[0])). It swaps the key-value pairs, creating a new RDD with count as the key and word as the value.\n\n#Sorting RDD by key: The sortByKey transformation is applied to rdd5 using rdd5.sortByKey(). It sorts the RDD elements based on the keys.\n\n#Collecting and printing RDD elements: The transformed RDD elements are collected using rdd5.collect(), and each element is printed.\n\n#Applying filter transformation: The filter transformation is applied to rdd5 using rdd5.filter(lambda x : 'a' in x[1]). It filters the RDD to only include elements where the word contains the letter 'a'.\n\n#Collecting and printing RDD elements: The filtered RDD elements are collected using rdd6.collect(), and each element is printed.\n\n#Importing functions from pyspark.sql.functions: The col and expr functions are imported from the pyspark.sql.functions module.\n\n#Creating a DataFrame and applying transformations: A list of tuples named data is defined. The list contains date and increment values. The list is used to create a DataFrame with column names \"date\" and \"increment\". The DataFrame is then selected with specific columns, and an expression is applied to add months to the date based on the increment value.\n\n#Displaying DataFrame: The resulting DataFrame is displayed using the show() method.\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cb25e560-649a-45da-b75f-2fe52f84007e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-rdd-wordcount-2.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
