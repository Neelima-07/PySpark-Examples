{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\nprint(spark)\nrdd=spark.sparkContext.parallelize([1,2,3,4,56])\nprint(\"RDD count :\"+str(rdd.count()))\n\nrdd = spark.sparkContext.emptyRDD\nprint(rdd)\nrdd2 = spark.sparkContext.parallelize([])\nprint(rdd2)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"25b22176-8799-4af7-9883-cb8d64280eea","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["<pyspark.sql.session.SparkSession object at 0x7f0841e59400>\nRDD count :5\n<bound method SparkContext.emptyRDD of <SparkContext master=local[8] appName=Databricks Shell>>\nParallelCollectionRDD[214] at readRDDFromInputStream at PythonRDD.scala:435\n"]}],"execution_count":0},{"cell_type":"code","source":["#Importing necessary modules: The SparkSession class is imported from the pyspark.sql module.\n\n#Creating a SparkSession: The SparkSession is created using the SparkSession.builder method with the application name set to 'SparkByExamples.com'. If a SparkSession already exists, it returns that instance; otherwise, it creates a new one.\n\n#Printing the SparkSession: The print(spark) statement prints the information about the SparkSession, including the application name and other configuration details.\n\n#Creating an RDD: An RDD named rdd is created using spark.sparkContext.parallelize([1,2,3,4,56]). The RDD is parallelized from the list of integers.\n\n#Printing the RDD count: The print(\"RDD count :\" + str(rdd.count())) statement prints the count of elements in the RDD using the count() action.\n\n#Creating an empty RDD: The emptyRDD variable is assigned the value of spark.sparkContext.emptyRDD. This creates an empty RDD with no elements.\n\n#Printing the empty RDD: The print(rdd) statement prints the information about the empty RDD.\n\n#Creating an empty parallelized RDD: The rdd2 variable is assigned the value of spark.sparkContext.parallelize([]). This creates an RDD with no elements using an empty list.\n\n#Printing the empty parallelized RDD: The print(rdd2) statement prints the information about the empty parallelized RDD."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"235c4cc5-965d-4fc5-bb16-8c73322afd02","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-rdd.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
