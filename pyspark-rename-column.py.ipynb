{"cells":[{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\nfrom pyspark.sql.functions import *\n\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\n\ndataDF = [(('James','','Smith'),'1991-04-01','M',3000),\n  (('Michael','Rose',''),'2000-05-19','M',4000),\n  (('Robert','','Williams'),'1978-09-05','M',4000),\n  (('Maria','Anne','Jones'),'1967-12-01','F',4000),\n  (('Jen','Mary','Brown'),'1980-02-17','F',-1)\n]\n\nschema = StructType([\n        StructField('name', StructType([\n             StructField('firstname', StringType(), True),\n             StructField('middlename', StringType(), True),\n             StructField('lastname', StringType(), True)\n             ])),\n         StructField('dob', StringType(), True),\n         StructField('gender', StringType(), True),\n         StructField('salary', IntegerType(), True)\n         ])\n\ndf = spark.createDataFrame(data = dataDF, schema = schema)\ndf.printSchema()\n\n''' Example 1 '''\ndf.withColumnRenamed(\"dob\",\"DateOfBirth\").printSchema()\n''' Example 2 '''    \ndf2 = df.withColumnRenamed(\"dob\",\"DateOfBirth\") \\\n    .withColumnRenamed(\"salary\",\"salary_amount\")\ndf2.printSchema()\n\n''' Example 3 '''\nschema2 = StructType([\n    StructField(\"fname\",StringType()),\n    StructField(\"middlename\",StringType()),\n    StructField(\"lname\",StringType())])\n    \ndf.select(col(\"name\").cast(schema2),\n  col(\"dob\"),\n  col(\"gender\"),\n  col(\"salary\")) \\\n    .printSchema()    \n\n''' Example 4 '''\ndf.select(col(\"name.firstname\").alias(\"fname\"),\n  col(\"name.middlename\").alias(\"mname\"),\n  col(\"name.lastname\").alias(\"lname\"),\n  col(\"dob\"),col(\"gender\"),col(\"salary\")) \\\n  .printSchema()\n  \n''' Example 5 '''  \ndf4 = df.withColumn(\"fname\",col(\"name.firstname\")) \\\n      .withColumn(\"mname\",col(\"name.middlename\")) \\\n      .withColumn(\"lname\",col(\"name.lastname\")) \\\n      .drop(\"name\")\ndf4.printSchema()\n\n''' Example 6\n\nnot working\nval old_columns = Seq(\"dob\",\"gender\",\"salary\",\"fname\",\"mname\",\"lname\")\n    val new_columns = Seq(\"DateOfBirth\",\"Sex\",\"salary\",\"firstName\",\"middleName\",\"lastName\")\n    val columnsList = old_columns.zip(new_columns).map(f=&gt;{col(f._1).as(f._2)})\n    val df5 = df4.select(columnsList:_*)\n    df5.printSchema()\n'''\n''' Example 7\nnot working \nnewColumns = [\"newCol1\",\"newCol2\",\"newCol3\",\"newCol4\"]\ndf.toDF(newColumns) \\\n.printSchema()\n'''        "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"99379e67-616d-41db-942e-9d80b74ff146","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\nroot\n |-- name: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- DateOfBirth: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\nroot\n |-- name: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- DateOfBirth: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary_amount: integer (nullable = true)\n\nroot\n |-- name: struct (nullable = true)\n |    |-- fname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lname: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\nroot\n |-- fname: string (nullable = true)\n |-- mname: string (nullable = true)\n |-- lname: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\nroot\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n |-- fname: string (nullable = true)\n |-- mname: string (nullable = true)\n |-- lname: string (nullable = true)\n\nOut[1]: ' Example 7\\nnot working \\nnewColumns = [\"newCol1\",\"newCol2\",\"newCol3\",\"newCol4\"]\\ndf.toDF(newColumns) .printSchema()\\n'"]}],"execution_count":0},{"cell_type":"code","source":["#A list of tuples, addData, represents address-related data.\n#A list of column names, addColumns, represents the column names for the \"addDF\" DataFrame.\n#The spark.createDataFrame() method is used to create the \"addDF\" DataFrame from the data and column names.\n#Finally, the show() method is called to display the content of the \"addDF\" DataFrame.\n#The join() method is used to join the \"empDF\" DataFrame and the \"addDF\" DataFrame based on the equality of the \"emp_id\" column.\n#The resulting DataFrame is displayed using the show() method.\n#The join() method is used to join the \"empDF\" DataFrame and the \"addDF\" DataFrame based on the equality of the \"emp_id\" column.\n#Since \"emp_id\" is a common column between the two DataFrames, it will be duplicated in the result. By passing [\"emp_id\"] as the join condition, the duplicate column is dropped, and only one instance of \"emp_id\" is included in the resulting DataFrame.\n#The resulting DataFrame is displayed using the show() method.\n#The join() method is used to join the \"empDF\" DataFrame and the \"addDF\" DataFrame based on the equality of the \"emp_id\" column.\n#The resulting DataFrame is then joined with the \"deptDF\" DataFrame based on the equality of the \"emp_dept_id\" column in \"empDF\" and the \"dept_id\" column in \"deptDF\".\n#The resulting DataFrame is displayed using the show() method.\n#The join() method is used to join the \"empDF\" DataFrame and the \"deptDF\" DataFrame without specifying a join condition.\n#The resulting DataFrame is then filtered using the where() method, specifying the join condition between the \"emp_dept_id\" column in \"empDF\" and the \"dept_id\" column in \"deptDF\"."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"329b2128-f7d3-40e3-b613-3e05f08a4168","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-rename-column.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
