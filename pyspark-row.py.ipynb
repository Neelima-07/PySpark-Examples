{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession, Row\n\n\nrow=Row(\"James\",40)\nprint(row[0] +\",\"+str(row[1]))\nrow2=Row(name=\"Alice\", age=11)\nprint(row2.name)\n\nPerson = Row(\"name\", \"age\")\np1=Person(\"James\", 40)\np2=Person(\"Alice\", 35)\nprint(p1.name +\",\"+p2.name)\n\n#PySpark Example\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\nrdd2 = spark.sparkContext.parallelize([],10)\n\ndata = [Row(name=\"James,,Smith\",lang=[\"Java\",\"Scala\",\"C++\"],state=\"CA\"), \n    Row(name=\"Michael,Rose,\",lang=[\"Spark\",\"Java\",\"C++\"],state=\"NJ\"),\n    Row(name=\"Robert,,Williams\",lang=[\"CSharp\",\"VB\"],state=\"NV\")]\n\n#RDD Example 1\nrdd=spark.sparkContext.parallelize(data)\ncollData=rdd.collect()\nprint(collData)\nfor row in collData:\n    print(row.name + \",\" +str(row.lang))\n\n# RDD Example 2\nPerson=Row(\"name\",\"lang\",\"state\")\ndata = [Person(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \n    Person(\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"),\n    Person(\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\nrdd=spark.sparkContext.parallelize(data)\ncollData=rdd.collect()\nprint(collData)\nfor person in collData:\n    print(person.name + \",\" +str(person.lang))\n\n#DataFrame Example 1\ncolumns = [\"name\",\"languagesAtSchool\",\"currentState\"]\ndf=spark.createDataFrame(data)\ndf.printSchema()\ndf.show()\n\ncollData=df.collect()\nprint(collData)\nfor row in collData:\n    print(row.name + \",\" +str(row.lang))\n    \n#DataFrame Example 2\ndata = [(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \n(\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"),\n(\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\ncolumns = [\"name\",\"languagesAtSchool\",\"currentState\"]\ndf=spark.createDataFrame(data).toDF(*columns)\ndf.printSchema()\nfor row in df.collect():\n    print(row.name)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"4aaa91ce-1342-4308-9414-2db65836201b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["James,40\nAlice\nJames,Alice\n[Row(name='James,,Smith', lang=['Java', 'Scala', 'C++'], state='CA'), Row(name='Michael,Rose,', lang=['Spark', 'Java', 'C++'], state='NJ'), Row(name='Robert,,Williams', lang=['CSharp', 'VB'], state='NV')]\nJames,,Smith,['Java', 'Scala', 'C++']\nMichael,Rose,,['Spark', 'Java', 'C++']\nRobert,,Williams,['CSharp', 'VB']\n[Row(name='James,,Smith', lang=['Java', 'Scala', 'C++'], state='CA'), Row(name='Michael,Rose,', lang=['Spark', 'Java', 'C++'], state='NJ'), Row(name='Robert,,Williams', lang=['CSharp', 'VB'], state='NV')]\nJames,,Smith,['Java', 'Scala', 'C++']\nMichael,Rose,,['Spark', 'Java', 'C++']\nRobert,,Williams,['CSharp', 'VB']\nroot\n |-- name: string (nullable = true)\n |-- lang: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- state: string (nullable = true)\n\n+----------------+------------------+-----+\n|            name|              lang|state|\n+----------------+------------------+-----+\n|    James,,Smith|[Java, Scala, C++]|   CA|\n|   Michael,Rose,|[Spark, Java, C++]|   NJ|\n|Robert,,Williams|      [CSharp, VB]|   NV|\n+----------------+------------------+-----+\n\n[Row(name='James,,Smith', lang=['Java', 'Scala', 'C++'], state='CA'), Row(name='Michael,Rose,', lang=['Spark', 'Java', 'C++'], state='NJ'), Row(name='Robert,,Williams', lang=['CSharp', 'VB'], state='NV')]\nJames,,Smith,['Java', 'Scala', 'C++']\nMichael,Rose,,['Spark', 'Java', 'C++']\nRobert,,Williams,['CSharp', 'VB']\nroot\n |-- name: string (nullable = true)\n |-- languagesAtSchool: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- currentState: string (nullable = true)\n\nJames,,Smith\nMichael,Rose,\nRobert,,Williams\n"]}],"execution_count":0},{"cell_type":"code","source":["#Creating Row Objects:\n\n#The code creates a Row object named row with two elements: \"James\" and 40. It then prints the values of the row.\n#Another Row object named row2 is created with named attributes: \"name\" and \"age\". It accesses and prints the value of the \"name\" attribute.\n#Creating Row Objects with Named Attributes:\n\n#The code defines a Row object type named \"Person\" with two attributes: \"name\" and \"age\".\n#Two instances of the \"Person\" object are created: p1 and p2, with corresponding values.\n#The code accesses and prints the values of the \"name\" attribute for p1 and p2.\n#PySpark RDD Examples:\n\n#A SparkSession is created.\n#An RDD named rdd2 is created as an empty RDD with 10 partitions.\n#Data is defined as a list of Row objects with attributes: \"name\", \"lang\", and \"state\".\n#Example 1: An RDD named rdd is created using parallelize with the data. It collects the RDD into a list named collData and prints its content.\n#Example 2: An RDD named rdd is created with the same data but with explicit definition using Row objects. It also collects the RDD into collData and prints its content.\n#DataFrame Examples:\n\n#Example 1: A DataFrame named df is created directly from the data list. It prints the schema and shows the DataFrame contents.\n#Example 2: The same data is used to create a DataFrame named df by specifying column names. It prints the schema and shows the DataFrame contents. It also demonstrates iterating over the DataFrame and accessing column values.\n#These examples demonstrate different ways to work with rows, RDDs, and DataFrames in PySpark, including creating Row objects, creating RDDs from Rows, and creating DataFrames from RDDs or lists of Rows.\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ccf4e8da-59ea-4155-a82c-7cf6312a6a72","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-row.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
