{"cells":[{"cell_type":"code","source":["\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .master(\"local[1]\") \\\n    .appName(\"SparkByExamples.com\") \\\n    .getOrCreate()\ndata = [('Scott', 50), ('Jeff', 45), ('Thomas', 54),('Ann',34)] \nsparkDF=spark.createDataFrame(data,[\"name\",\"age\"]) \nsparkDF.printSchema()\nsparkDF.show()\n\nprint((sparkDF.count(), len(sparkDF.columns)))\n\ndef sparkShape(dataFrame):\n    return (dataFrame.count(), len(dataFrame.columns))\nimport pyspark\npyspark.sql.dataframe.DataFrame.shape = sparkShape\nprint(sparkDF.shape())\n\nimport pandas as pd    \npandasDF=sparkDF.toPandas()\nprint(pandasDF.shape)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"5415da82-c75a-4b70-b00f-3410d479085f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name: string (nullable = true)\n |-- age: long (nullable = true)\n\n+------+---+\n|  name|age|\n+------+---+\n| Scott| 50|\n|  Jeff| 45|\n|Thomas| 54|\n|   Ann| 34|\n+------+---+\n\n(4, 2)\n(4, 2)\n"]},{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/plain":[],"application/vnd.databricks.v1+bamboolib_hint":"{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}"}},{"output_type":"stream","output_type":"stream","name":"stdout","text":["(4, 2)\n"]}],"execution_count":0},{"cell_type":"code","source":["#The code starts by importing the necessary modules: SparkSession from pyspark.sql and pandas for working with Pandas DataFrames.\n\n#A SparkSession is created using the SparkSession.builder API. The master parameter is set to \"local[1]\", which runs Spark locally with one worker thread. The appName parameter sets the name of the Spark application.\n\n#The code defines a sample data as a list of tuples. Each tuple represents a row in the DataFrame and contains a name and an age.\n\n#Using the spark.createDataFrame() method, a Spark DataFrame named sparkDF is created from the sample data. The data parameter is set to the input data, and the [\"name\", \"age\"] parameter specifies the column names for the DataFrame.\n\n#The sparkDF.printSchema() statement prints the schema of the DataFrame, which shows the column names and their corresponding data types.\n\n#The sparkDF.show() statement displays the contents of the DataFrame.\n\n#The print((sparkDF.count(), len(sparkDF.columns))) line prints the shape of the DataFrame, which is determined by the number of rows (obtained using count()) and the number of columns (obtained using len() on the columns attribute).\n\n#The sparkShape() function is defined as a custom function to calculate the shape of a DataFrame. It takes a DataFrame as input and returns a tuple containing the number of rows and the number of columns.\n\n#The code extends the DataFrame class in PySpark by adding a shape() method. This is achieved by assigning the sparkShape() function to the shape attribute of the DataFrame class.\n\n#The print(sparkDF.shape()) line calls the shape() method on the sparkDF DataFrame and prints its shape, which is determined using the custom sparkShape() function.\n\n#Finally, the sparkDF DataFrame is converted to a Pandas DataFrame named pandasDF using the toPandas() method.\n\n#The print(pandasDF.shape) statement prints the shape of the Pandas DataFrame, which is determined using the shape attribute of the Pandas DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"414e2bd9-56b3-4170-bd9f-f6d7b3e2ef27","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-shape-dataframe.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
