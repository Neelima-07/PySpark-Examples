{"cells":[{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import split, col\nspark=SparkSession.builder.appName(\"sparkbyexamples\").getOrCreate()\n\ndata=data = [('James','','Smith','1991-04-01'),\n  ('Michael','Rose','','2000-05-19'),\n  ('Robert','','Williams','1978-09-05'),\n  ('Maria','Anne','Jones','1967-12-01'),\n  ('Jen','Mary','Brown','1980-02-17')\n]\n\ncolumns=[\"firstname\",\"middlename\",\"lastname\",\"dob\"]\ndf=spark.createDataFrame(data,columns)\ndf.printSchema()\ndf.show(truncate=False)\ndf1 = df.withColumn('year', split(df['dob'], '-').getItem(0)) \\\n       .withColumn('month', split(df['dob'], '-').getItem(1)) \\\n       .withColumn('day', split(df['dob'], '-').getItem(2))\ndf1.printSchema()\ndf1.show(truncate=False)\n\n # Alternatively we can do like below      \nsplit_col = pyspark.sql.functions.split(df['dob'], '-')\ndf2 = df.withColumn('year', split_col.getItem(0)) \\\n       .withColumn('month', split_col.getItem(1)) \\\n       .withColumn('day', split_col.getItem(2))\ndf2.show(truncate=False)      \n\n# Using split() function of Column class\nsplit_col = pyspark.sql.functions.split(df['dob'], '-')\ndf3 = df.select(\"firstname\",\"middlename\",\"lastname\",\"dob\", split_col.getItem(0).alias('year'),split_col.getItem(1).alias('month'),split_col.getItem(2).alias('day'))   \ndf3.show(truncate=False)\n\n\"\"\"\ndf4=spark.createDataFrame([(\"20-13-2012-monday\",)], ['date',])\n\ndf4.select(split(df4.date,'^([\\d]+-[\\d]+-[\\d])').alias('date'),\n    regexp_replace(split(df4.date,'^([\\d]+-[\\d]+-[\\d]+)').getItem(1),'-','').alias('day')).show()\n    \"\"\"\ndf4 = spark.createDataFrame([('oneAtwoBthree',)], ['str',])\ndf4.select(split(df4.str, '[AB]').alias('str')).show()\n\ndf4.select(split(df4.str, '[AB]',2).alias('str')).show()\ndf4.select(split(df4.str, '[AB]',1).alias('str')).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"4a7e3885-5024-4e65-a960-5ea030b53f36","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- dob: string (nullable = true)\n\n+---------+----------+--------+----------+\n|firstname|middlename|lastname|dob       |\n+---------+----------+--------+----------+\n|James    |          |Smith   |1991-04-01|\n|Michael  |Rose      |        |2000-05-19|\n|Robert   |          |Williams|1978-09-05|\n|Maria    |Anne      |Jones   |1967-12-01|\n|Jen      |Mary      |Brown   |1980-02-17|\n+---------+----------+--------+----------+\n\nroot\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- year: string (nullable = true)\n |-- month: string (nullable = true)\n |-- day: string (nullable = true)\n\n+---------+----------+--------+----------+----+-----+---+\n|firstname|middlename|lastname|dob       |year|month|day|\n+---------+----------+--------+----------+----+-----+---+\n|James    |          |Smith   |1991-04-01|1991|04   |01 |\n|Michael  |Rose      |        |2000-05-19|2000|05   |19 |\n|Robert   |          |Williams|1978-09-05|1978|09   |05 |\n|Maria    |Anne      |Jones   |1967-12-01|1967|12   |01 |\n|Jen      |Mary      |Brown   |1980-02-17|1980|02   |17 |\n+---------+----------+--------+----------+----+-----+---+\n\n+---------+----------+--------+----------+----+-----+---+\n|firstname|middlename|lastname|dob       |year|month|day|\n+---------+----------+--------+----------+----+-----+---+\n|James    |          |Smith   |1991-04-01|1991|04   |01 |\n|Michael  |Rose      |        |2000-05-19|2000|05   |19 |\n|Robert   |          |Williams|1978-09-05|1978|09   |05 |\n|Maria    |Anne      |Jones   |1967-12-01|1967|12   |01 |\n|Jen      |Mary      |Brown   |1980-02-17|1980|02   |17 |\n+---------+----------+--------+----------+----+-----+---+\n\n+---------+----------+--------+----------+----+-----+---+\n|firstname|middlename|lastname|dob       |year|month|day|\n+---------+----------+--------+----------+----+-----+---+\n|James    |          |Smith   |1991-04-01|1991|04   |01 |\n|Michael  |Rose      |        |2000-05-19|2000|05   |19 |\n|Robert   |          |Williams|1978-09-05|1978|09   |05 |\n|Maria    |Anne      |Jones   |1967-12-01|1967|12   |01 |\n|Jen      |Mary      |Brown   |1980-02-17|1980|02   |17 |\n+---------+----------+--------+----------+----+-----+---+\n\n+-----------------+\n|              str|\n+-----------------+\n|[one, two, three]|\n+-----------------+\n\n+----------------+\n|             str|\n+----------------+\n|[one, twoBthree]|\n+----------------+\n\n+---------------+\n|            str|\n+---------------+\n|[oneAtwoBthree]|\n+---------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#The code starts by importing the necessary modules: pyspark, SparkSession, and split and col functions from pyspark.sql.functions. These modules are required for creating a SparkSession, working with Spark DataFrames, and using the split() and col() functions.\n\n#A SparkSession is created using the SparkSession.builder API. The appName parameter sets the name of the Spark application. If an existing SparkSession with the same name exists, it will be retrieved; otherwise, a new SparkSession will be created.\n\n#Sample data is defined as a list of tuples. Each tuple represents a row of data, with elements corresponding to the columns: firstname, middlename, lastname, and dob (date of birth).\n\n#A Spark DataFrame is created using spark.createDataFrame(data, columns), where data is the sample data and columns is the list of column names.\n\n#The DataFrame schema is printed using df.printSchema(), which displays the data types of each column.\n\n#The contents of the DataFrame are displayed using df.show(truncate=False), which shows all the rows and columns without truncation.\n\n#The split() function is used to split the dob column into multiple columns: year, month, and day. The split() function is applied using the withColumn() method on the DataFrame. This is done in two different ways: first by accessing split() directly from the pyspark.sql.functions module, and then by using the split() function of the Column class obtained from df['dob'].\n\n#The resulting DataFrames with the split columns are printed using df1.show(truncate=False) and df2.show(truncate=False).\n\n#Another approach to splitting the dob column is demonstrated using the select() method and the split() function of the Column class. This approach creates a new DataFrame df3 with the original columns along with the split columns year, month, and day. The resulting DataFrame is printed using df3.show(truncate=False).\n\n#The code snippet also includes some commented-out code that demonstrates the usage of the split() and regexp_replace() functions for more complex splitting and data manipulation scenarios.\n\n#Finally, the code demonstrates the usage of the split() function on a different DataFrame df4 to split a string column based on a delimiter [AB]. The resulting DataFrame is printed using df4.select(split(df4.str, '[AB]').alias('str')).show(), along with examples of specifying the maximum number of splits using the split() function."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a5336f83-2923-4878-8450-08f9d8abc0b5","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-split-function.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
