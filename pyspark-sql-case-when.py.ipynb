{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\ndata = [(\"James\",\"M\",60000), (\"Michael\",\"M\",70000),\n        (\"Robert\",None,400000), (\"Maria\",\"F\",500000),\n        (\"Jen\",\"\",None)]\n\ncolumns = [\"name\",\"gender\",\"salary\"]\ndf = spark.createDataFrame(data = data, schema = columns)\ndf.show()\n\n#Using When otherwise\nfrom pyspark.sql.functions import when,col\ndf2 = df.withColumn(\"new_gender\", when(df.gender == \"M\",\"Male\")\n                                 .when(df.gender == \"F\",\"Female\")\n                                 .when(df.gender.isNull() ,\"\")\n                                 .otherwise(df.gender))\ndf2.show()\ndf2 = df.withColumn(\"new_gender\", when(df.gender == \"M\",\"Male\")\n                                 .when(df.gender == \"F\",\"Female\")\n                                 .when(df.gender.isNull() ,\"\")\n                                 .otherwise(df.gender))\n\ndf2=df.select(col(\"*\"),when(df.gender == \"M\",\"Male\")\n                  .when(df.gender == \"F\",\"Female\")\n                  .when(df.gender.isNull() ,\"\")\n                  .otherwise(df.gender).alias(\"new_gender\"))\ndf2.show()\n# Using SQL Case When\nfrom pyspark.sql.functions import expr\ndf3 = df.withColumn(\"new_gender\", expr(\"CASE WHEN gender = 'M' THEN 'Male' \" + \n           \"WHEN gender = 'F' THEN 'Female' WHEN gender IS NULL THEN ''\" +\n          \"ELSE gender END\"))\ndf3.show()\n\ndf4 = df.select(col(\"*\"), expr(\"CASE WHEN gender = 'M' THEN 'Male' \" +\n           \"WHEN gender = 'F' THEN 'Female' WHEN gender IS NULL THEN ''\" +\n           \"ELSE gender END\").alias(\"new_gender\"))\n\ndf.createOrReplaceTempView(\"EMP\")\nspark.sql(\"select name, CASE WHEN gender = 'M' THEN 'Male' \" + \n               \"WHEN gender = 'F' THEN 'Female' WHEN gender IS NULL THEN ''\" +\n              \"ELSE gender END as new_gender from EMP\").show"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"76a09739-42e5-4f03-bdb7-b6d236133212","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------+------+------+\n|   name|gender|salary|\n+-------+------+------+\n|  James|     M| 60000|\n|Michael|     M| 70000|\n| Robert|  null|400000|\n|  Maria|     F|500000|\n|    Jen|      |  null|\n+-------+------+------+\n\n+-------+------+------+----------+\n|   name|gender|salary|new_gender|\n+-------+------+------+----------+\n|  James|     M| 60000|      Male|\n|Michael|     M| 70000|      Male|\n| Robert|  null|400000|          |\n|  Maria|     F|500000|    Female|\n|    Jen|      |  null|          |\n+-------+------+------+----------+\n\n+-------+------+------+----------+\n|   name|gender|salary|new_gender|\n+-------+------+------+----------+\n|  James|     M| 60000|      Male|\n|Michael|     M| 70000|      Male|\n| Robert|  null|400000|          |\n|  Maria|     F|500000|    Female|\n|    Jen|      |  null|          |\n+-------+------+------+----------+\n\n+-------+------+------+----------+\n|   name|gender|salary|new_gender|\n+-------+------+------+----------+\n|  James|     M| 60000|      Male|\n|Michael|     M| 70000|      Male|\n| Robert|  null|400000|          |\n|  Maria|     F|500000|    Female|\n|    Jen|      |  null|          |\n+-------+------+------+----------+\n\nOut[1]: <bound method DataFrame.show of DataFrame[name: string, new_gender: string]>"]}],"execution_count":0},{"cell_type":"code","source":["#The code starts by importing the necessary modules: SparkSession from pyspark.sql and when, col, and expr functions from pyspark.sql.functions. These modules are required for creating a SparkSession, working with Spark DataFrames, and performing conditional transformations.\n\n#A SparkSession is created using the SparkSession.builder API. The appName parameter sets the name of the Spark application. If an existing SparkSession with the same name exists, it will be retrieved; otherwise, a new SparkSession will be created.\n\n#Sample data is defined as a list of tuples. Each tuple represents a row of data, with elements corresponding to the columns: name, gender, and salary.\n\n#The schema of the DataFrame is defined using the schema parameter when creating the DataFrame using spark.createDataFrame(data, schema). This ensures that the columns have the correct data types.\n\n#The contents of the DataFrame are displayed using df.show(), which shows all the rows and columns.\n\n#The code demonstrates different approaches to perform conditional transformations on the gender column and create a new column new_gender based on specific conditions.\n\n#Approach 1: Using when() function chained with otherwise() function\n#Approach 2: Using withColumn() and when() function\n#Approach 3: Using expr() function with SQL-like CASE WHEN statement\n#Approach 4: Using SQL syntax with select() and alias()\n#The resulting DataFrames with the new new_gender column are displayed using df2.show() and df3.show().\n\n#Lastly, the code demonstrates how to perform the same conditional transformation using Spark SQL. It creates a temporary view named \"EMP\" with df.createOrReplaceTempView(\"EMP\") and then executes a SQL query on the temporary view using spark.sql(). The resulting DataFrame is displayed using .show().\n\n#These different approaches allow for flexible conditional transformations on Spark DataFrames, providing options to derive new columns based on specific conditions.\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"764bcaae-55e8-4c8e-a800-6a2923d1f74c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-sql-case-when.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
