{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder \\\n         .appName('SparkByExamples.com') \\\n         .getOrCreate()\n\ndata = [(\"James, A, Smith\",\"2018\",\"M\",3000),\n            (\"Michael, Rose, Jones\",\"2010\",\"M\",4000),\n            (\"Robert,K,Williams\",\"2010\",\"M\",4000),\n            (\"Maria,Anne,Jones\",\"2005\",\"F\",4000),\n            (\"Jen,Mary,Brown\",\"2010\",\"\",-1)\n            ]\n\ncolumns=[\"name\",\"dob_year\",\"gender\",\"salary\"]\ndf=spark.createDataFrame(data,columns)\ndf.printSchema()\ndf.show(truncate=False)\n\nfrom pyspark.sql.functions import split, col\ndf2 = df.select(split(col(\"name\"),\",\").alias(\"NameArray\")) \\\n    .drop(\"name\")\ndf2.printSchema()\ndf2.show()\n\ndf.createOrReplaceTempView(\"PERSON\")\nspark.sql(\"select SPLIT(name,',') as NameArray from PERSON\") \\\n    .show()\n    "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"20458a15-718b-41cc-8625-851781a3f550","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name: string (nullable = true)\n |-- dob_year: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+--------------------+--------+------+------+\n|name                |dob_year|gender|salary|\n+--------------------+--------+------+------+\n|James, A, Smith     |2018    |M     |3000  |\n|Michael, Rose, Jones|2010    |M     |4000  |\n|Robert,K,Williams   |2010    |M     |4000  |\n|Maria,Anne,Jones    |2005    |F     |4000  |\n|Jen,Mary,Brown      |2010    |      |-1    |\n+--------------------+--------+------+------+\n\nroot\n |-- NameArray: array (nullable = true)\n |    |-- element: string (containsNull = false)\n\n+--------------------+\n|           NameArray|\n+--------------------+\n| [James,  A,  Smith]|\n|[Michael,  Rose, ...|\n|[Robert, K, Willi...|\n|[Maria, Anne, Jones]|\n|  [Jen, Mary, Brown]|\n+--------------------+\n\n+--------------------+\n|           NameArray|\n+--------------------+\n| [James,  A,  Smith]|\n|[Michael,  Rose, ...|\n|[Robert, K, Willi...|\n|[Maria, Anne, Jones]|\n|  [Jen, Mary, Brown]|\n+--------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#The code begins by importing the SparkSession class from the pyspark.sql module and creating a SparkSession using the SparkSession.builder API. The appName parameter sets the name of the Spark application. If an existing SparkSession with the same name exists, it will be retrieved; otherwise, a new SparkSession will be created.\n\n#A list of tuples called data is defined, representing the rows of the DataFrame. Each tuple contains values for the columns \"name\", \"dob_year\", \"gender\", and \"salary\". This data is used to create the DataFrame.\n\n#The spark.createDataFrame() method is used to create a DataFrame named df from the data list and the specified column names in the columns list.\n\n#The schema of the DataFrame is printed using the printSchema() method.\n\n#The content of the DataFrame is displayed using the show() method with truncate=False, which ensures that all column values are fully displayed.\n\n#The split() function is imported from pyspark.sql.functions. This function splits the values in the \"name\" column based on the comma delimiter and returns an array of substrings.\n\n#The df.select() method is used to select the \"name\" column and apply the split() function to it. The resulting column is named \"NameArray\". The original \"name\" column is dropped using the drop() method.\n\n#The schema of the new DataFrame df2 is printed using the printSchema() method.\n\n#The content of the new DataFrame df2 is displayed using the show() method.\n\n#The original DataFrame df is registered as a temporary view named \"PERSON\" using the createOrReplaceTempView() method.\n\n#A Spark SQL query is executed using the spark.sql() method. The query selects the \"name\" column and applies the SPLIT() function to it. The resulting column is named \"NameArray\". The query results are displayed using the show() method.\n\n#Overall, this code showcases how to split a string column into an array of substrings using the split() function in PySpark, both with DataFrame transformations and Spark SQL queries.\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0a41bded-dcf6-4aca-89b7-7bc5215cde62","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-string-to-array.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
