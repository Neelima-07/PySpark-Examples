{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\ndata = [ (\"36636\",\"Finance\",(3000,\"USA\")), \n    (\"40288\",\"Finance\",(5000,\"IND\")), \n    (\"42114\",\"Sales\",(3900,\"USA\")), \n    (\"39192\",\"Marketing\",(2500,\"CAN\")), \n    (\"34534\",\"Sales\",(6500,\"USA\")) ]\nschema = StructType([\n     StructField('id', StringType(), True),\n     StructField('dept', StringType(), True),\n     StructField('properties', StructType([\n         StructField('salary', IntegerType(), True),\n         StructField('location', StringType(), True)\n         ]))\n     ])\n\ndf = spark.createDataFrame(data=data,schema=schema)\ndf.printSchema()\ndf.show(truncate=False)\n\n\n#Convert struct type to Map\nfrom pyspark.sql.functions import col,lit,create_map\ndf = df.withColumn(\"propertiesMap\",create_map(\n        lit(\"salary\"),col(\"properties.salary\"),\n        lit(\"location\"),col(\"properties.location\")\n        )).drop(\"properties\")\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"3164238d-2c96-4aef-9a8a-80a7c02b1dd1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- id: string (nullable = true)\n |-- dept: string (nullable = true)\n |-- properties: struct (nullable = true)\n |    |-- salary: integer (nullable = true)\n |    |-- location: string (nullable = true)\n\n+-----+---------+-----------+\n|id   |dept     |properties |\n+-----+---------+-----------+\n|36636|Finance  |{3000, USA}|\n|40288|Finance  |{5000, IND}|\n|42114|Sales    |{3900, USA}|\n|39192|Marketing|{2500, CAN}|\n|34534|Sales    |{6500, USA}|\n+-----+---------+-----------+\n\nroot\n |-- id: string (nullable = true)\n |-- dept: string (nullable = true)\n |-- propertiesMap: map (nullable = false)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n+-----+---------+---------------------------------+\n|id   |dept     |propertiesMap                    |\n+-----+---------+---------------------------------+\n|36636|Finance  |{salary -> 3000, location -> USA}|\n|40288|Finance  |{salary -> 5000, location -> IND}|\n|42114|Sales    |{salary -> 3900, location -> USA}|\n|39192|Marketing|{salary -> 2500, location -> CAN}|\n|34534|Sales    |{salary -> 6500, location -> USA}|\n+-----+---------+---------------------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#The code starts by importing the necessary modules: SparkSession from pyspark.sql and various types and functions from pyspark.sql.types and pyspark.sql.functions.\n\n#A SparkSession is created using the SparkSession.builder API with the application name set to 'SparkByExamples.com'.\n\n#The data is defined as a list of tuples, representing the rows of the DataFrame. Each tuple contains values for the columns \"id\", \"dept\", and \"properties\". The \"properties\" column itself is a struct type, with nested fields \"salary\" and \"location\".\n\n#The schema is defined using the StructType class from pyspark.sql.types. The schema specifies the structure of the DataFrame, including the data types and nested fields.\n\n#The DataFrame is created using the spark.createDataFrame() method, passing the data and schema as arguments. The resulting DataFrame is named df.\n\n#The schema of the DataFrame is printed using the printSchema() method.\n\n#The content of the DataFrame is displayed using the show() method with truncate=False, ensuring that all column values are fully displayed.\n\n#The create_map() function from pyspark.sql.functions is used to convert the struct type column \"properties\" into a map type column named \"propertiesMap\". The create_map() function takes pairs of literal values and column references and creates a map from them. In this case, the pairs consist of the literal strings \"salary\" and \"location\" as keys, and the respective column references \"properties.salary\" and \"properties.location\" as values.\n\n#The original \"properties\" column is dropped using the drop() method.\n\n#The schema of the updated DataFrame is printed using the printSchema() method.\n\n#The content of the updated DataFrame is displayed using the show() method.\n\n#Overall, this code showcases how to convert a struct type column into a map type column in PySpark by using the create_map() function. It provides a way to transform and manipulate nested structures within DataFrames.\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d0324e01-5dec-487d-9493-27aa7ff26dd2","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-struct-to-map.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
