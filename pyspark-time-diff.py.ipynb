{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\n\n# Create SparkSession\nspark = SparkSession.builder \\\n          .appName('SparkByExamples.com') \\\n          .getOrCreate()\n\n\ndates = [(\"1\",\"2019-07-01 12:01:19.111\"),\n    (\"2\",\"2019-06-24 12:01:19.222\"),\n    (\"3\",\"2019-11-16 16:44:55.406\"),\n    (\"4\",\"2019-11-16 16:50:59.406\")\n    ]\n\ndf = spark.createDataFrame(data=dates, schema=[\"id\",\"from_timestamp\"])\n\nfrom pyspark.sql.functions import *\ndf2=df.withColumn('from_timestamp',to_timestamp(col('from_timestamp')))\\\n  .withColumn('end_timestamp', current_timestamp())\\\n  .withColumn('DiffInSeconds',col(\"end_timestamp\").cast(\"long\") - col('from_timestamp').cast(\"long\"))\ndf2.show(truncate=False)\n\ndf.withColumn('from_timestamp',to_timestamp(col('from_timestamp')))\\\n  .withColumn('end_timestamp', current_timestamp())\\\n  .withColumn('DiffInSeconds',unix_timestamp(\"end_timestamp\") - unix_timestamp('from_timestamp')) \\\n  .show(truncate=False)\n\ndf2.withColumn('DiffInMinutes',round(col('DiffInSeconds')/60))\\\n  .show(truncate=False)\n  \ndf2.withColumn('DiffInHours',round(col('DiffInSeconds')/3600))\\\n  .show(truncate=False)\n  \n#Difference between two timestamps when input has just timestamp\n\ndata= [(\"12:01:19.000\",\"13:01:19.000\"),\n    (\"12:01:19.000\",\"12:02:19.000\"),\n    (\"16:44:55.406\",\"17:44:55.406\"),\n    (\"16:50:59.406\",\"16:44:59.406\")]\ndf3 = spark.createDataFrame(data=data, schema=[\"from_timestamp\",\"to_timestamp\"])\n\ndf3.withColumn(\"from_timestamp\",to_timestamp(col(\"from_timestamp\"),\"HH:mm:ss.SSS\")) \\\n   .withColumn(\"to_timestamp\",to_timestamp(col(\"to_timestamp\"),\"HH:mm:ss.SSS\")) \\\n   .withColumn(\"DiffInSeconds\", col(\"from_timestamp\").cast(\"long\") - col(\"to_timestamp\").cast(\"long\")) \\\n   .withColumn(\"DiffInMinutes\",round(col(\"DiffInSeconds\")/60)) \\\n   .withColumn(\"DiffInHours\",round(col(\"DiffInSeconds\")/3600)) \\\n   .show(truncate=False)\n   \n#\n\n\ndf3 = spark.createDataFrame(\n        data=[(\"1\",\"07-01-2019 12:01:19.406\")], \n        schema=[\"id\",\"input_timestamp\"]\n        )\ndf3.withColumn(\"input_timestamp\",to_timestamp(col(\"input_timestamp\"),\"MM-dd-yyyy HH:mm:ss.SSS\")) \\\n    .withColumn(\"current_timestamp\",current_timestamp().alias(\"current_timestamp\")) \\\n    .withColumn(\"DiffInSeconds\",current_timestamp().cast(\"long\") - col(\"input_timestamp\").cast(\"long\")) \\\n    .withColumn(\"DiffInMinutes\",round(col(\"DiffInSeconds\")/60)) \\\n    .withColumn(\"DiffInHours\",round(col(\"DiffInSeconds\")/3600)) \\\n    .withColumn(\"DiffInDays\",round(col(\"DiffInSeconds\")/24*3600)) \\\n    .show(truncate=False)\n    \n#SQL\n\nspark.sql(\"select unix_timestamp('2019-07-02 12:01:19') - unix_timestamp('2019-07-01 12:01:19') DiffInSeconds\").show()\nspark.sql(\"select (unix_timestamp('2019-07-02 12:01:19') - unix_timestamp('2019-07-01 12:01:19'))/60 DiffInMinutes\").show()\nspark.sql(\"select (unix_timestamp('2019-07-02 12:01:19') - unix_timestamp('2019-07-01 12:01:19'))/3600 DiffInHours\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"3f105684-6aac-4cb1-ac69-cf7cf814c9be","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-----------------------+-----------------------+-------------+\n|id |from_timestamp         |end_timestamp          |DiffInSeconds|\n+---+-----------------------+-----------------------+-------------+\n|1  |2019-07-01 12:01:19.111|2023-06-21 16:54:32.462|125383993    |\n|2  |2019-06-24 12:01:19.222|2023-06-21 16:54:32.462|125988793    |\n|3  |2019-11-16 16:44:55.406|2023-06-21 16:54:32.462|113443777    |\n|4  |2019-11-16 16:50:59.406|2023-06-21 16:54:32.462|113443413    |\n+---+-----------------------+-----------------------+-------------+\n\n+---+-----------------------+-----------------------+-------------+\n|id |from_timestamp         |end_timestamp          |DiffInSeconds|\n+---+-----------------------+-----------------------+-------------+\n|1  |2019-07-01 12:01:19.111|2023-06-21 16:54:33.373|125383994    |\n|2  |2019-06-24 12:01:19.222|2023-06-21 16:54:33.373|125988794    |\n|3  |2019-11-16 16:44:55.406|2023-06-21 16:54:33.373|113443778    |\n|4  |2019-11-16 16:50:59.406|2023-06-21 16:54:33.373|113443414    |\n+---+-----------------------+-----------------------+-------------+\n\n+---+-----------------------+-----------------------+-------------+-------------+\n|id |from_timestamp         |end_timestamp          |DiffInSeconds|DiffInMinutes|\n+---+-----------------------+-----------------------+-------------+-------------+\n|1  |2019-07-01 12:01:19.111|2023-06-21 16:54:33.928|125383994    |2089733.0    |\n|2  |2019-06-24 12:01:19.222|2023-06-21 16:54:33.928|125988794    |2099813.0    |\n|3  |2019-11-16 16:44:55.406|2023-06-21 16:54:33.928|113443778    |1890730.0    |\n|4  |2019-11-16 16:50:59.406|2023-06-21 16:54:33.928|113443414    |1890724.0    |\n+---+-----------------------+-----------------------+-------------+-------------+\n\n+---+-----------------------+-----------------------+-------------+-----------+\n|id |from_timestamp         |end_timestamp          |DiffInSeconds|DiffInHours|\n+---+-----------------------+-----------------------+-------------+-----------+\n|1  |2019-07-01 12:01:19.111|2023-06-21 16:54:34.454|125383995    |34829.0    |\n|2  |2019-06-24 12:01:19.222|2023-06-21 16:54:34.454|125988795    |34997.0    |\n|3  |2019-11-16 16:44:55.406|2023-06-21 16:54:34.454|113443779    |31512.0    |\n|4  |2019-11-16 16:50:59.406|2023-06-21 16:54:34.454|113443415    |31512.0    |\n+---+-----------------------+-----------------------+-------------+-----------+\n\n+-----------------------+-----------------------+-------------+-------------+-----------+\n|from_timestamp         |to_timestamp           |DiffInSeconds|DiffInMinutes|DiffInHours|\n+-----------------------+-----------------------+-------------+-------------+-----------+\n|1970-01-01 12:01:19    |1970-01-01 13:01:19    |-3600        |-60.0        |-1.0       |\n|1970-01-01 12:01:19    |1970-01-01 12:02:19    |-60          |-1.0         |0.0        |\n|1970-01-01 16:44:55.406|1970-01-01 17:44:55.406|-3600        |-60.0        |-1.0       |\n|1970-01-01 16:50:59.406|1970-01-01 16:44:59.406|360          |6.0          |0.0        |\n+-----------------------+-----------------------+-------------+-------------+-----------+\n\n+---+-----------------------+-----------------------+-------------+-------------+-----------+--------------+\n|id |input_timestamp        |current_timestamp      |DiffInSeconds|DiffInMinutes|DiffInHours|DiffInDays    |\n+---+-----------------------+-----------------------+-------------+-------------+-----------+--------------+\n|1  |2019-07-01 12:01:19.406|2023-06-21 16:54:36.101|125383997    |2089733.0    |34829.0    |1.880759955E10|\n+---+-----------------------+-----------------------+-------------+-------------+-----------+--------------+\n\n+-------------+\n|DiffInSeconds|\n+-------------+\n|        86400|\n+-------------+\n\n+-------------+\n|DiffInMinutes|\n+-------------+\n|       1440.0|\n+-------------+\n\n+-----------+\n|DiffInHours|\n+-----------+\n|       24.0|\n+-----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#The code imports necessary modules, including SparkSession from pyspark.sql and various functions related to timestamp manipulation.\n\n#A SparkSession is created with the application name set to 'SparkByExamples.com'.\n\n#Two DataFrames are created: df and df3, representing different timestamp scenarios.\n\n#Operations on df:\n\n#The from_timestamp column is converted to a timestamp type using the to_timestamp() function.\n#The end_timestamp column is set to the current timestamp using the current_timestamp() function.\n#The DiffInSeconds column is computed by calculating the difference between end_timestamp and from_timestamp in seconds.\n#Similar operations are performed on df, but with additional calculations of differences in minutes and hours.\n\n#Operations on df3:\n\n#The from_timestamp and to_timestamp columns are converted to timestamp types using the to_timestamp() function.\n#The DiffInSeconds column is computed by calculating the difference between from_timestamp and to_timestamp in seconds.\n#Similar calculations are performed to obtain differences in minutes and hours.\n#The DataFrames df2, df3, and their calculated columns are displayed using the show() method.\n\n#SQL operations:\n\n#The code uses Spark SQL to perform similar calculations as above, using the unix_timestamp() function to convert timestamps to Unix timestamps and then calculating the differences in seconds, minutes, and hours.\n#Overall, this code showcases different ways to work with timestamps in PySpark. It demonstrates how to convert string representations of timestamps to actual timestamp types, calculate differences between timestamps, and perform various operations using both DataFrame functions and Spark SQL.\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0131d3c3-14cb-4474-941d-49b8c9392df1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-time-diff.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
