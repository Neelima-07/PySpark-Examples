{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\n\n# Create SparkSession\nspark = SparkSession.builder \\\n          .appName('SparkByExamples.com') \\\n          .getOrCreate()\n\ninputData = [(\"2019-07-01 12:01:19\",\n            \"07-01-2019 12:01:19\", \n            \"07-01-2019\")]\ncolumns=[\"timestamp_1\",\"timestamp_2\",\"timestamp_3\"]\ndf=spark.createDataFrame(\n        data = inputData,\n        schema = columns)\ndf.printSchema()\ndf.show(truncate=False)\n\nfrom pyspark.sql.functions import *\ndf2 = df.select( \n      unix_timestamp(col(\"timestamp_1\")).alias(\"timestamp_1\"), \n      unix_timestamp(col(\"timestamp_2\"),\"MM-dd-yyyy HH:mm:ss\").alias(\"timestamp_2\"), \n      unix_timestamp(col(\"timestamp_3\"),\"MM-dd-yyyy\").alias(\"timestamp_3\"), \n      unix_timestamp().alias(\"timestamp_4\") \n   )\ndf2.printSchema()\ndf2.show(truncate=False)\n\ndf3=df2.select(\n    from_unixtime(col(\"timestamp_1\")).alias(\"timestamp_1\"),\n    from_unixtime(col(\"timestamp_2\"),\"MM-dd-yyyy HH:mm:ss\").alias(\"timestamp_2\"),\n    from_unixtime(col(\"timestamp_3\"),\"MM-dd-yyyy\").alias(\"timestamp_3\"),\n    from_unixtime(col(\"timestamp_4\")).alias(\"timestamp_4\")\n  )\ndf3.printSchema()\ndf3.show(truncate=False)\n\n#SQL"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"4050e530-d39d-4936-93ca-5e7ff343b0e4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- timestamp_1: string (nullable = true)\n |-- timestamp_2: string (nullable = true)\n |-- timestamp_3: string (nullable = true)\n\n+-------------------+-------------------+-----------+\n|timestamp_1        |timestamp_2        |timestamp_3|\n+-------------------+-------------------+-----------+\n|2019-07-01 12:01:19|07-01-2019 12:01:19|07-01-2019 |\n+-------------------+-------------------+-----------+\n\nroot\n |-- timestamp_1: long (nullable = true)\n |-- timestamp_2: long (nullable = true)\n |-- timestamp_3: long (nullable = true)\n |-- timestamp_4: long (nullable = true)\n\n+-----------+-----------+-----------+-----------+\n|timestamp_1|timestamp_2|timestamp_3|timestamp_4|\n+-----------+-----------+-----------+-----------+\n|1561982479 |1561982479 |1561939200 |1687361628 |\n+-----------+-----------+-----------+-----------+\n\nroot\n |-- timestamp_1: string (nullable = true)\n |-- timestamp_2: string (nullable = true)\n |-- timestamp_3: string (nullable = true)\n |-- timestamp_4: string (nullable = true)\n\n+-------------------+-------------------+-----------+-------------------+\n|timestamp_1        |timestamp_2        |timestamp_3|timestamp_4        |\n+-------------------+-------------------+-----------+-------------------+\n|2019-07-01 12:01:19|07-01-2019 12:01:19|07-01-2019 |2023-06-21 15:33:49|\n+-------------------+-------------------+-----------+-------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#The code imports the necessary modules, including SparkSession from pyspark.sql.\n\n#A SparkSession is created with the application name set to 'SparkByExamples.com'.\n\n#The inputData list contains tuples representing input data for the DataFrame.\n\n#The columns list defines the column names for the DataFrame.\n\n#The DataFrame df is created using spark.createDataFrame() by passing the input data and schema.\n\n#The schema and contents of df are printed using printSchema() and displayed using show().\n\n#The unix_timestamp() function from pyspark.sql.functions is used to convert the timestamp columns in df to Unix timestamps. New columns timestamp_1, timestamp_2, timestamp_3, and timestamp_4 are created in the DataFrame df2 using select().\n\n#The schema and contents of df2 are printed using printSchema() and displayed using show().\n\n#The from_unixtime() function is used to convert the Unix timestamp columns in df2 back to timestamp strings. The resulting DataFrame is assigned to df3.\n\n#The schema and contents of df3 are printed using printSchema() and displayed using show().\n\n#The code then demonstrates the usage of SQL operations. SQL queries are executed using the spark.sql() method to perform operations on the DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b20a30aa-6dde-4e32-97fa-5a717279c700","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-unix-time.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
